{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML2021Spring - HW1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx3x1nDkG-Uy"
      },
      "source": [
        "# **Download Data**\n",
        "\n",
        "\n",
        "If the Google drive links are dead, you can download data from [kaggle](https://www.kaggle.com/c/ml2021spring-hw1/data), and upload data manually to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMj55YDKG6ch",
        "outputId": "cd9ec9a0-f8e8-4ac9-887b-2c343b1d6cca"
      },
      "source": [
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 31.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 86.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "myseed = 40000  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtE3b6JEH7rw"
      },
      "source": [
        "# **Some Utilities**\n",
        "\n",
        "You do not need to modify this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWMT3uf1NGQp"
      },
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "    ''' Plot prediction of your DNN '''\n",
        "    if preds is None or targets is None:\n",
        "        model.eval()\n",
        "        preds, targets = [], []\n",
        "        for x, y in dv_set:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = model(x)\n",
        "                preds.append(pred.detach().cpu())\n",
        "                targets.append(y.detach().cpu())\n",
        "        preds = torch.cat(preds, dim=0).numpy()\n",
        "        targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    plt.xlabel('ground truth value')\n",
        "    plt.ylabel('predicted value')\n",
        "    plt.title('Ground Truth v.s. Prediction')\n",
        "    plt.show()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39U_XFX6KOoj"
      },
      "source": [
        "# **Preprocess**\n",
        "\n",
        "We have three kinds of datasets:\n",
        "* `train`: for training\n",
        "* `dev`: for validation\n",
        "* `test`: for testing (w/o target value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ-MdwpLL7Dt"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "The `COVID19Dataset` below does:\n",
        "* read `.csv` files\n",
        "* extract features\n",
        "* split `covid.train.csv` into train/dev sets\n",
        "* normalize features\n",
        "\n",
        "Finishing `TODO` below might make you pass medium baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zlpIp9ANJRU"
      },
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    def __init__(self,\n",
        "                 path,\n",
        "                 mode='train',\n",
        "                 target_only=False):\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "        \n",
        "        if not target_only:\n",
        "            feats = list(range(93))\n",
        "        else:\n",
        "            #Using 40 states & high relevance features (indices = 57, 52, 41~44, 48, 50, 56) \n",
        "            feats = [41, 42, 43, 44, 48, 50, 52, 56, 57, 59, 60, 61, 62, 66, 68, 70, 74, 75]\n",
        "            pass\n",
        "\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets\n",
        "            if mode == 'train':\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
        "            \n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        # Normalize features (you may remove this part to see what will happen)\n",
        "        self.data[:, :40] = \\\n",
        "            (self.data[:, :40] - self.data[:, :40].mean(dim=0, keepdim=True)) \\\n",
        "            / self.data[:, :40].std(dim=0, keepdim=True)\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhTlkE7MDo3"
      },
      "source": [
        "## **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhLk5t6MBX3"
      },
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuycwR0MeQB"
      },
      "source": [
        "# **Deep Neural Network**\n",
        "\n",
        "`NeuralNet` is an `nn.Module` designed for regression.\n",
        "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
        "This module also included a function `cal_loss` for calculating loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-uXYovOAI0"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim,128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(128,32),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        # TODO: you may implement L2 regularization here\n",
        "        loss = self.criterion(pred, target)\n",
        "        '''\n",
        "        l2_lambda = 0.001\n",
        "        l2_reg = torch.tensor(0.).to('cuda')\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param)\n",
        "        loss += l2_lambda * l2_reg\n",
        "        '''\n",
        "        return loss"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvFWVjZ5Nvga"
      },
      "source": [
        "# **Train/Dev/Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAM8QecJOyqn"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOqcmYzMO7jB"
      },
      "source": [
        "def train(tr_set, dv_set, model, config, device):\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(),**config['optim_hparas'])\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            optimizer.zero_grad()               # set gradient to zero\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            print('Saving model (epoch = {:4d}, loss = {:.4f})'\n",
        "                .format(epoch + 1, min_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hSd4Bn3O2PL"
      },
      "source": [
        "## **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrxrD3YsN3U2"
      },
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0pdrhQAO41L"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSBMRFlYN5tB"
      },
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvckkF5dvf0j"
      },
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPXpdumwPjE7"
      },
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = True                   # TODO: Using 40 states & 2 tested_positive features\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 6000,                # maximum number of epochs\n",
        "    'batch_size': 512,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.0001,                 # learning rate of SGD\n",
        "        'weight_decay': 0.1               # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 2000,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1eOV3TOH-j"
      },
      "source": [
        "# **Load data and model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNrYBMmePLKm",
        "outputId": "0cb23f19-055c-4819-d18c-5d4bbec08ce6"
      },
      "source": [
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 18)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 18)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHylSirLP9oh"
      },
      "source": [
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      },
      "source": [
        "# **Start Training!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrEbUxazQAAZ",
        "outputId": "95ebc864-0160-4987-f083-becc833401c4"
      },
      "source": [
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model (epoch =    1, loss = 331.1576)\n",
            "Saving model (epoch =    2, loss = 330.2450)\n",
            "Saving model (epoch =    3, loss = 329.3305)\n",
            "Saving model (epoch =    4, loss = 328.4137)\n",
            "Saving model (epoch =    5, loss = 327.4878)\n",
            "Saving model (epoch =    6, loss = 326.5443)\n",
            "Saving model (epoch =    7, loss = 325.5870)\n",
            "Saving model (epoch =    8, loss = 324.6088)\n",
            "Saving model (epoch =    9, loss = 323.6077)\n",
            "Saving model (epoch =   10, loss = 322.5757)\n",
            "Saving model (epoch =   11, loss = 321.5175)\n",
            "Saving model (epoch =   12, loss = 320.4300)\n",
            "Saving model (epoch =   13, loss = 319.3198)\n",
            "Saving model (epoch =   14, loss = 318.1931)\n",
            "Saving model (epoch =   15, loss = 317.0395)\n",
            "Saving model (epoch =   16, loss = 315.8565)\n",
            "Saving model (epoch =   17, loss = 314.6478)\n",
            "Saving model (epoch =   18, loss = 313.3951)\n",
            "Saving model (epoch =   19, loss = 312.1055)\n",
            "Saving model (epoch =   20, loss = 310.7696)\n",
            "Saving model (epoch =   21, loss = 309.3841)\n",
            "Saving model (epoch =   22, loss = 307.9623)\n",
            "Saving model (epoch =   23, loss = 306.4927)\n",
            "Saving model (epoch =   24, loss = 304.9839)\n",
            "Saving model (epoch =   25, loss = 303.4418)\n",
            "Saving model (epoch =   26, loss = 301.8528)\n",
            "Saving model (epoch =   27, loss = 300.2307)\n",
            "Saving model (epoch =   28, loss = 298.5639)\n",
            "Saving model (epoch =   29, loss = 296.8592)\n",
            "Saving model (epoch =   30, loss = 295.1098)\n",
            "Saving model (epoch =   31, loss = 293.3110)\n",
            "Saving model (epoch =   32, loss = 291.4626)\n",
            "Saving model (epoch =   33, loss = 289.5692)\n",
            "Saving model (epoch =   34, loss = 287.6203)\n",
            "Saving model (epoch =   35, loss = 285.6149)\n",
            "Saving model (epoch =   36, loss = 283.5509)\n",
            "Saving model (epoch =   37, loss = 281.4395)\n",
            "Saving model (epoch =   38, loss = 279.2638)\n",
            "Saving model (epoch =   39, loss = 277.0401)\n",
            "Saving model (epoch =   40, loss = 274.7550)\n",
            "Saving model (epoch =   41, loss = 272.4163)\n",
            "Saving model (epoch =   42, loss = 270.0209)\n",
            "Saving model (epoch =   43, loss = 267.5554)\n",
            "Saving model (epoch =   44, loss = 265.0420)\n",
            "Saving model (epoch =   45, loss = 262.4664)\n",
            "Saving model (epoch =   46, loss = 259.8427)\n",
            "Saving model (epoch =   47, loss = 257.1536)\n",
            "Saving model (epoch =   48, loss = 254.3938)\n",
            "Saving model (epoch =   49, loss = 251.5903)\n",
            "Saving model (epoch =   50, loss = 248.7083)\n",
            "Saving model (epoch =   51, loss = 245.7785)\n",
            "Saving model (epoch =   52, loss = 242.7988)\n",
            "Saving model (epoch =   53, loss = 239.7742)\n",
            "Saving model (epoch =   54, loss = 236.6785)\n",
            "Saving model (epoch =   55, loss = 233.5156)\n",
            "Saving model (epoch =   56, loss = 230.3000)\n",
            "Saving model (epoch =   57, loss = 227.0334)\n",
            "Saving model (epoch =   58, loss = 223.7276)\n",
            "Saving model (epoch =   59, loss = 220.3797)\n",
            "Saving model (epoch =   60, loss = 216.9751)\n",
            "Saving model (epoch =   61, loss = 213.5174)\n",
            "Saving model (epoch =   62, loss = 210.0159)\n",
            "Saving model (epoch =   63, loss = 206.4811)\n",
            "Saving model (epoch =   64, loss = 202.9078)\n",
            "Saving model (epoch =   65, loss = 199.2775)\n",
            "Saving model (epoch =   66, loss = 195.6484)\n",
            "Saving model (epoch =   67, loss = 191.9558)\n",
            "Saving model (epoch =   68, loss = 188.2643)\n",
            "Saving model (epoch =   69, loss = 184.5497)\n",
            "Saving model (epoch =   70, loss = 180.7975)\n",
            "Saving model (epoch =   71, loss = 177.0327)\n",
            "Saving model (epoch =   72, loss = 173.2478)\n",
            "Saving model (epoch =   73, loss = 169.4665)\n",
            "Saving model (epoch =   74, loss = 165.6815)\n",
            "Saving model (epoch =   75, loss = 161.8875)\n",
            "Saving model (epoch =   76, loss = 158.1252)\n",
            "Saving model (epoch =   77, loss = 154.3718)\n",
            "Saving model (epoch =   78, loss = 150.6119)\n",
            "Saving model (epoch =   79, loss = 146.8790)\n",
            "Saving model (epoch =   80, loss = 143.1549)\n",
            "Saving model (epoch =   81, loss = 139.4639)\n",
            "Saving model (epoch =   82, loss = 135.8158)\n",
            "Saving model (epoch =   83, loss = 132.1932)\n",
            "Saving model (epoch =   84, loss = 128.5991)\n",
            "Saving model (epoch =   85, loss = 125.0433)\n",
            "Saving model (epoch =   86, loss = 121.5207)\n",
            "Saving model (epoch =   87, loss = 118.0668)\n",
            "Saving model (epoch =   88, loss = 114.6521)\n",
            "Saving model (epoch =   89, loss = 111.2874)\n",
            "Saving model (epoch =   90, loss = 107.9900)\n",
            "Saving model (epoch =   91, loss = 104.7448)\n",
            "Saving model (epoch =   92, loss = 101.5749)\n",
            "Saving model (epoch =   93, loss = 98.4671)\n",
            "Saving model (epoch =   94, loss = 95.4293)\n",
            "Saving model (epoch =   95, loss = 92.4664)\n",
            "Saving model (epoch =   96, loss = 89.5746)\n",
            "Saving model (epoch =   97, loss = 86.7668)\n",
            "Saving model (epoch =   98, loss = 84.0429)\n",
            "Saving model (epoch =   99, loss = 81.3818)\n",
            "Saving model (epoch =  100, loss = 78.8141)\n",
            "Saving model (epoch =  101, loss = 76.3218)\n",
            "Saving model (epoch =  102, loss = 73.9072)\n",
            "Saving model (epoch =  103, loss = 71.5770)\n",
            "Saving model (epoch =  104, loss = 69.3279)\n",
            "Saving model (epoch =  105, loss = 67.1598)\n",
            "Saving model (epoch =  106, loss = 65.0777)\n",
            "Saving model (epoch =  107, loss = 63.0799)\n",
            "Saving model (epoch =  108, loss = 61.1692)\n",
            "Saving model (epoch =  109, loss = 59.3393)\n",
            "Saving model (epoch =  110, loss = 57.5845)\n",
            "Saving model (epoch =  111, loss = 55.9074)\n",
            "Saving model (epoch =  112, loss = 54.3080)\n",
            "Saving model (epoch =  113, loss = 52.7834)\n",
            "Saving model (epoch =  114, loss = 51.3361)\n",
            "Saving model (epoch =  115, loss = 49.9463)\n",
            "Saving model (epoch =  116, loss = 48.6323)\n",
            "Saving model (epoch =  117, loss = 47.3876)\n",
            "Saving model (epoch =  118, loss = 46.1993)\n",
            "Saving model (epoch =  119, loss = 45.0763)\n",
            "Saving model (epoch =  120, loss = 44.0170)\n",
            "Saving model (epoch =  121, loss = 43.0117)\n",
            "Saving model (epoch =  122, loss = 42.0662)\n",
            "Saving model (epoch =  123, loss = 41.1614)\n",
            "Saving model (epoch =  124, loss = 40.3148)\n",
            "Saving model (epoch =  125, loss = 39.5110)\n",
            "Saving model (epoch =  126, loss = 38.7593)\n",
            "Saving model (epoch =  127, loss = 38.0422)\n",
            "Saving model (epoch =  128, loss = 37.3762)\n",
            "Saving model (epoch =  129, loss = 36.7460)\n",
            "Saving model (epoch =  130, loss = 36.1472)\n",
            "Saving model (epoch =  131, loss = 35.5809)\n",
            "Saving model (epoch =  132, loss = 35.0432)\n",
            "Saving model (epoch =  133, loss = 34.5349)\n",
            "Saving model (epoch =  134, loss = 34.0571)\n",
            "Saving model (epoch =  135, loss = 33.6026)\n",
            "Saving model (epoch =  136, loss = 33.1753)\n",
            "Saving model (epoch =  137, loss = 32.7634)\n",
            "Saving model (epoch =  138, loss = 32.3794)\n",
            "Saving model (epoch =  139, loss = 32.0060)\n",
            "Saving model (epoch =  140, loss = 31.6542)\n",
            "Saving model (epoch =  141, loss = 31.3221)\n",
            "Saving model (epoch =  142, loss = 31.0019)\n",
            "Saving model (epoch =  143, loss = 30.6938)\n",
            "Saving model (epoch =  144, loss = 30.4003)\n",
            "Saving model (epoch =  145, loss = 30.1159)\n",
            "Saving model (epoch =  146, loss = 29.8441)\n",
            "Saving model (epoch =  147, loss = 29.5859)\n",
            "Saving model (epoch =  148, loss = 29.3340)\n",
            "Saving model (epoch =  149, loss = 29.0913)\n",
            "Saving model (epoch =  150, loss = 28.8579)\n",
            "Saving model (epoch =  151, loss = 28.6297)\n",
            "Saving model (epoch =  152, loss = 28.4116)\n",
            "Saving model (epoch =  153, loss = 28.2003)\n",
            "Saving model (epoch =  154, loss = 27.9932)\n",
            "Saving model (epoch =  155, loss = 27.7924)\n",
            "Saving model (epoch =  156, loss = 27.5983)\n",
            "Saving model (epoch =  157, loss = 27.4077)\n",
            "Saving model (epoch =  158, loss = 27.2244)\n",
            "Saving model (epoch =  159, loss = 27.0454)\n",
            "Saving model (epoch =  160, loss = 26.8696)\n",
            "Saving model (epoch =  161, loss = 26.6983)\n",
            "Saving model (epoch =  162, loss = 26.5293)\n",
            "Saving model (epoch =  163, loss = 26.3631)\n",
            "Saving model (epoch =  164, loss = 26.2034)\n",
            "Saving model (epoch =  165, loss = 26.0445)\n",
            "Saving model (epoch =  166, loss = 25.8890)\n",
            "Saving model (epoch =  167, loss = 25.7356)\n",
            "Saving model (epoch =  168, loss = 25.5848)\n",
            "Saving model (epoch =  169, loss = 25.4376)\n",
            "Saving model (epoch =  170, loss = 25.2907)\n",
            "Saving model (epoch =  171, loss = 25.1490)\n",
            "Saving model (epoch =  172, loss = 25.0092)\n",
            "Saving model (epoch =  173, loss = 24.8720)\n",
            "Saving model (epoch =  174, loss = 24.7353)\n",
            "Saving model (epoch =  175, loss = 24.6016)\n",
            "Saving model (epoch =  176, loss = 24.4686)\n",
            "Saving model (epoch =  177, loss = 24.3385)\n",
            "Saving model (epoch =  178, loss = 24.2097)\n",
            "Saving model (epoch =  179, loss = 24.0832)\n",
            "Saving model (epoch =  180, loss = 23.9599)\n",
            "Saving model (epoch =  181, loss = 23.8358)\n",
            "Saving model (epoch =  182, loss = 23.7143)\n",
            "Saving model (epoch =  183, loss = 23.5929)\n",
            "Saving model (epoch =  184, loss = 23.4739)\n",
            "Saving model (epoch =  185, loss = 23.3564)\n",
            "Saving model (epoch =  186, loss = 23.2387)\n",
            "Saving model (epoch =  187, loss = 23.1228)\n",
            "Saving model (epoch =  188, loss = 23.0089)\n",
            "Saving model (epoch =  189, loss = 22.8965)\n",
            "Saving model (epoch =  190, loss = 22.7849)\n",
            "Saving model (epoch =  191, loss = 22.6748)\n",
            "Saving model (epoch =  192, loss = 22.5654)\n",
            "Saving model (epoch =  193, loss = 22.4569)\n",
            "Saving model (epoch =  194, loss = 22.3506)\n",
            "Saving model (epoch =  195, loss = 22.2443)\n",
            "Saving model (epoch =  196, loss = 22.1388)\n",
            "Saving model (epoch =  197, loss = 22.0348)\n",
            "Saving model (epoch =  198, loss = 21.9308)\n",
            "Saving model (epoch =  199, loss = 21.8278)\n",
            "Saving model (epoch =  200, loss = 21.7250)\n",
            "Saving model (epoch =  201, loss = 21.6236)\n",
            "Saving model (epoch =  202, loss = 21.5232)\n",
            "Saving model (epoch =  203, loss = 21.4240)\n",
            "Saving model (epoch =  204, loss = 21.3252)\n",
            "Saving model (epoch =  205, loss = 21.2274)\n",
            "Saving model (epoch =  206, loss = 21.1307)\n",
            "Saving model (epoch =  207, loss = 21.0340)\n",
            "Saving model (epoch =  208, loss = 20.9379)\n",
            "Saving model (epoch =  209, loss = 20.8429)\n",
            "Saving model (epoch =  210, loss = 20.7489)\n",
            "Saving model (epoch =  211, loss = 20.6570)\n",
            "Saving model (epoch =  212, loss = 20.5648)\n",
            "Saving model (epoch =  213, loss = 20.4717)\n",
            "Saving model (epoch =  214, loss = 20.3801)\n",
            "Saving model (epoch =  215, loss = 20.2888)\n",
            "Saving model (epoch =  216, loss = 20.1978)\n",
            "Saving model (epoch =  217, loss = 20.1076)\n",
            "Saving model (epoch =  218, loss = 20.0179)\n",
            "Saving model (epoch =  219, loss = 19.9282)\n",
            "Saving model (epoch =  220, loss = 19.8397)\n",
            "Saving model (epoch =  221, loss = 19.7513)\n",
            "Saving model (epoch =  222, loss = 19.6633)\n",
            "Saving model (epoch =  223, loss = 19.5760)\n",
            "Saving model (epoch =  224, loss = 19.4887)\n",
            "Saving model (epoch =  225, loss = 19.4019)\n",
            "Saving model (epoch =  226, loss = 19.3161)\n",
            "Saving model (epoch =  227, loss = 19.2302)\n",
            "Saving model (epoch =  228, loss = 19.1447)\n",
            "Saving model (epoch =  229, loss = 19.0592)\n",
            "Saving model (epoch =  230, loss = 18.9745)\n",
            "Saving model (epoch =  231, loss = 18.8906)\n",
            "Saving model (epoch =  232, loss = 18.8069)\n",
            "Saving model (epoch =  233, loss = 18.7237)\n",
            "Saving model (epoch =  234, loss = 18.6411)\n",
            "Saving model (epoch =  235, loss = 18.5591)\n",
            "Saving model (epoch =  236, loss = 18.4768)\n",
            "Saving model (epoch =  237, loss = 18.3948)\n",
            "Saving model (epoch =  238, loss = 18.3137)\n",
            "Saving model (epoch =  239, loss = 18.2333)\n",
            "Saving model (epoch =  240, loss = 18.1524)\n",
            "Saving model (epoch =  241, loss = 18.0733)\n",
            "Saving model (epoch =  242, loss = 17.9944)\n",
            "Saving model (epoch =  243, loss = 17.9163)\n",
            "Saving model (epoch =  244, loss = 17.8371)\n",
            "Saving model (epoch =  245, loss = 17.7593)\n",
            "Saving model (epoch =  246, loss = 17.6818)\n",
            "Saving model (epoch =  247, loss = 17.6038)\n",
            "Saving model (epoch =  248, loss = 17.5279)\n",
            "Saving model (epoch =  249, loss = 17.4512)\n",
            "Saving model (epoch =  250, loss = 17.3756)\n",
            "Saving model (epoch =  251, loss = 17.3012)\n",
            "Saving model (epoch =  252, loss = 17.2263)\n",
            "Saving model (epoch =  253, loss = 17.1514)\n",
            "Saving model (epoch =  254, loss = 17.0774)\n",
            "Saving model (epoch =  255, loss = 17.0045)\n",
            "Saving model (epoch =  256, loss = 16.9307)\n",
            "Saving model (epoch =  257, loss = 16.8576)\n",
            "Saving model (epoch =  258, loss = 16.7846)\n",
            "Saving model (epoch =  259, loss = 16.7125)\n",
            "Saving model (epoch =  260, loss = 16.6408)\n",
            "Saving model (epoch =  261, loss = 16.5692)\n",
            "Saving model (epoch =  262, loss = 16.4975)\n",
            "Saving model (epoch =  263, loss = 16.4268)\n",
            "Saving model (epoch =  264, loss = 16.3570)\n",
            "Saving model (epoch =  265, loss = 16.2873)\n",
            "Saving model (epoch =  266, loss = 16.2175)\n",
            "Saving model (epoch =  267, loss = 16.1485)\n",
            "Saving model (epoch =  268, loss = 16.0793)\n",
            "Saving model (epoch =  269, loss = 16.0113)\n",
            "Saving model (epoch =  270, loss = 15.9426)\n",
            "Saving model (epoch =  271, loss = 15.8747)\n",
            "Saving model (epoch =  272, loss = 15.8072)\n",
            "Saving model (epoch =  273, loss = 15.7401)\n",
            "Saving model (epoch =  274, loss = 15.6733)\n",
            "Saving model (epoch =  275, loss = 15.6071)\n",
            "Saving model (epoch =  276, loss = 15.5416)\n",
            "Saving model (epoch =  277, loss = 15.4753)\n",
            "Saving model (epoch =  278, loss = 15.4096)\n",
            "Saving model (epoch =  279, loss = 15.3450)\n",
            "Saving model (epoch =  280, loss = 15.2796)\n",
            "Saving model (epoch =  281, loss = 15.2145)\n",
            "Saving model (epoch =  282, loss = 15.1509)\n",
            "Saving model (epoch =  283, loss = 15.0867)\n",
            "Saving model (epoch =  284, loss = 15.0240)\n",
            "Saving model (epoch =  285, loss = 14.9608)\n",
            "Saving model (epoch =  286, loss = 14.8980)\n",
            "Saving model (epoch =  287, loss = 14.8349)\n",
            "Saving model (epoch =  288, loss = 14.7727)\n",
            "Saving model (epoch =  289, loss = 14.7100)\n",
            "Saving model (epoch =  290, loss = 14.6485)\n",
            "Saving model (epoch =  291, loss = 14.5866)\n",
            "Saving model (epoch =  292, loss = 14.5252)\n",
            "Saving model (epoch =  293, loss = 14.4637)\n",
            "Saving model (epoch =  294, loss = 14.4030)\n",
            "Saving model (epoch =  295, loss = 14.3426)\n",
            "Saving model (epoch =  296, loss = 14.2830)\n",
            "Saving model (epoch =  297, loss = 14.2227)\n",
            "Saving model (epoch =  298, loss = 14.1629)\n",
            "Saving model (epoch =  299, loss = 14.1032)\n",
            "Saving model (epoch =  300, loss = 14.0438)\n",
            "Saving model (epoch =  301, loss = 13.9854)\n",
            "Saving model (epoch =  302, loss = 13.9262)\n",
            "Saving model (epoch =  303, loss = 13.8685)\n",
            "Saving model (epoch =  304, loss = 13.8106)\n",
            "Saving model (epoch =  305, loss = 13.7520)\n",
            "Saving model (epoch =  306, loss = 13.6940)\n",
            "Saving model (epoch =  307, loss = 13.6362)\n",
            "Saving model (epoch =  308, loss = 13.5791)\n",
            "Saving model (epoch =  309, loss = 13.5223)\n",
            "Saving model (epoch =  310, loss = 13.4655)\n",
            "Saving model (epoch =  311, loss = 13.4089)\n",
            "Saving model (epoch =  312, loss = 13.3528)\n",
            "Saving model (epoch =  313, loss = 13.2963)\n",
            "Saving model (epoch =  314, loss = 13.2402)\n",
            "Saving model (epoch =  315, loss = 13.1849)\n",
            "Saving model (epoch =  316, loss = 13.1297)\n",
            "Saving model (epoch =  317, loss = 13.0749)\n",
            "Saving model (epoch =  318, loss = 13.0209)\n",
            "Saving model (epoch =  319, loss = 12.9665)\n",
            "Saving model (epoch =  320, loss = 12.9120)\n",
            "Saving model (epoch =  321, loss = 12.8580)\n",
            "Saving model (epoch =  322, loss = 12.8038)\n",
            "Saving model (epoch =  323, loss = 12.7503)\n",
            "Saving model (epoch =  324, loss = 12.6967)\n",
            "Saving model (epoch =  325, loss = 12.6446)\n",
            "Saving model (epoch =  326, loss = 12.5918)\n",
            "Saving model (epoch =  327, loss = 12.5397)\n",
            "Saving model (epoch =  328, loss = 12.4876)\n",
            "Saving model (epoch =  329, loss = 12.4358)\n",
            "Saving model (epoch =  330, loss = 12.3845)\n",
            "Saving model (epoch =  331, loss = 12.3334)\n",
            "Saving model (epoch =  332, loss = 12.2828)\n",
            "Saving model (epoch =  333, loss = 12.2324)\n",
            "Saving model (epoch =  334, loss = 12.1826)\n",
            "Saving model (epoch =  335, loss = 12.1332)\n",
            "Saving model (epoch =  336, loss = 12.0836)\n",
            "Saving model (epoch =  337, loss = 12.0345)\n",
            "Saving model (epoch =  338, loss = 11.9850)\n",
            "Saving model (epoch =  339, loss = 11.9355)\n",
            "Saving model (epoch =  340, loss = 11.8865)\n",
            "Saving model (epoch =  341, loss = 11.8378)\n",
            "Saving model (epoch =  342, loss = 11.7890)\n",
            "Saving model (epoch =  343, loss = 11.7414)\n",
            "Saving model (epoch =  344, loss = 11.6930)\n",
            "Saving model (epoch =  345, loss = 11.6450)\n",
            "Saving model (epoch =  346, loss = 11.5977)\n",
            "Saving model (epoch =  347, loss = 11.5507)\n",
            "Saving model (epoch =  348, loss = 11.5035)\n",
            "Saving model (epoch =  349, loss = 11.4573)\n",
            "Saving model (epoch =  350, loss = 11.4106)\n",
            "Saving model (epoch =  351, loss = 11.3642)\n",
            "Saving model (epoch =  352, loss = 11.3183)\n",
            "Saving model (epoch =  353, loss = 11.2725)\n",
            "Saving model (epoch =  354, loss = 11.2266)\n",
            "Saving model (epoch =  355, loss = 11.1813)\n",
            "Saving model (epoch =  356, loss = 11.1366)\n",
            "Saving model (epoch =  357, loss = 11.0921)\n",
            "Saving model (epoch =  358, loss = 11.0476)\n",
            "Saving model (epoch =  359, loss = 11.0027)\n",
            "Saving model (epoch =  360, loss = 10.9585)\n",
            "Saving model (epoch =  361, loss = 10.9145)\n",
            "Saving model (epoch =  362, loss = 10.8708)\n",
            "Saving model (epoch =  363, loss = 10.8270)\n",
            "Saving model (epoch =  364, loss = 10.7848)\n",
            "Saving model (epoch =  365, loss = 10.7414)\n",
            "Saving model (epoch =  366, loss = 10.6990)\n",
            "Saving model (epoch =  367, loss = 10.6563)\n",
            "Saving model (epoch =  368, loss = 10.6137)\n",
            "Saving model (epoch =  369, loss = 10.5717)\n",
            "Saving model (epoch =  370, loss = 10.5297)\n",
            "Saving model (epoch =  371, loss = 10.4879)\n",
            "Saving model (epoch =  372, loss = 10.4470)\n",
            "Saving model (epoch =  373, loss = 10.4057)\n",
            "Saving model (epoch =  374, loss = 10.3642)\n",
            "Saving model (epoch =  375, loss = 10.3237)\n",
            "Saving model (epoch =  376, loss = 10.2838)\n",
            "Saving model (epoch =  377, loss = 10.2438)\n",
            "Saving model (epoch =  378, loss = 10.2037)\n",
            "Saving model (epoch =  379, loss = 10.1642)\n",
            "Saving model (epoch =  380, loss = 10.1251)\n",
            "Saving model (epoch =  381, loss = 10.0862)\n",
            "Saving model (epoch =  382, loss = 10.0472)\n",
            "Saving model (epoch =  383, loss = 10.0087)\n",
            "Saving model (epoch =  384, loss = 9.9706)\n",
            "Saving model (epoch =  385, loss = 9.9325)\n",
            "Saving model (epoch =  386, loss = 9.8945)\n",
            "Saving model (epoch =  387, loss = 9.8572)\n",
            "Saving model (epoch =  388, loss = 9.8196)\n",
            "Saving model (epoch =  389, loss = 9.7823)\n",
            "Saving model (epoch =  390, loss = 9.7454)\n",
            "Saving model (epoch =  391, loss = 9.7087)\n",
            "Saving model (epoch =  392, loss = 9.6725)\n",
            "Saving model (epoch =  393, loss = 9.6360)\n",
            "Saving model (epoch =  394, loss = 9.5998)\n",
            "Saving model (epoch =  395, loss = 9.5638)\n",
            "Saving model (epoch =  396, loss = 9.5282)\n",
            "Saving model (epoch =  397, loss = 9.4927)\n",
            "Saving model (epoch =  398, loss = 9.4575)\n",
            "Saving model (epoch =  399, loss = 9.4228)\n",
            "Saving model (epoch =  400, loss = 9.3884)\n",
            "Saving model (epoch =  401, loss = 9.3540)\n",
            "Saving model (epoch =  402, loss = 9.3198)\n",
            "Saving model (epoch =  403, loss = 9.2858)\n",
            "Saving model (epoch =  404, loss = 9.2512)\n",
            "Saving model (epoch =  405, loss = 9.2175)\n",
            "Saving model (epoch =  406, loss = 9.1834)\n",
            "Saving model (epoch =  407, loss = 9.1502)\n",
            "Saving model (epoch =  408, loss = 9.1176)\n",
            "Saving model (epoch =  409, loss = 9.0847)\n",
            "Saving model (epoch =  410, loss = 9.0519)\n",
            "Saving model (epoch =  411, loss = 9.0193)\n",
            "Saving model (epoch =  412, loss = 8.9866)\n",
            "Saving model (epoch =  413, loss = 8.9544)\n",
            "Saving model (epoch =  414, loss = 8.9226)\n",
            "Saving model (epoch =  415, loss = 8.8913)\n",
            "Saving model (epoch =  416, loss = 8.8603)\n",
            "Saving model (epoch =  417, loss = 8.8289)\n",
            "Saving model (epoch =  418, loss = 8.7977)\n",
            "Saving model (epoch =  419, loss = 8.7667)\n",
            "Saving model (epoch =  420, loss = 8.7364)\n",
            "Saving model (epoch =  421, loss = 8.7056)\n",
            "Saving model (epoch =  422, loss = 8.6753)\n",
            "Saving model (epoch =  423, loss = 8.6451)\n",
            "Saving model (epoch =  424, loss = 8.6153)\n",
            "Saving model (epoch =  425, loss = 8.5856)\n",
            "Saving model (epoch =  426, loss = 8.5560)\n",
            "Saving model (epoch =  427, loss = 8.5265)\n",
            "Saving model (epoch =  428, loss = 8.4972)\n",
            "Saving model (epoch =  429, loss = 8.4683)\n",
            "Saving model (epoch =  430, loss = 8.4395)\n",
            "Saving model (epoch =  431, loss = 8.4108)\n",
            "Saving model (epoch =  432, loss = 8.3822)\n",
            "Saving model (epoch =  433, loss = 8.3544)\n",
            "Saving model (epoch =  434, loss = 8.3266)\n",
            "Saving model (epoch =  435, loss = 8.2988)\n",
            "Saving model (epoch =  436, loss = 8.2712)\n",
            "Saving model (epoch =  437, loss = 8.2436)\n",
            "Saving model (epoch =  438, loss = 8.2164)\n",
            "Saving model (epoch =  439, loss = 8.1895)\n",
            "Saving model (epoch =  440, loss = 8.1623)\n",
            "Saving model (epoch =  441, loss = 8.1354)\n",
            "Saving model (epoch =  442, loss = 8.1086)\n",
            "Saving model (epoch =  443, loss = 8.0821)\n",
            "Saving model (epoch =  444, loss = 8.0555)\n",
            "Saving model (epoch =  445, loss = 8.0292)\n",
            "Saving model (epoch =  446, loss = 8.0029)\n",
            "Saving model (epoch =  447, loss = 7.9774)\n",
            "Saving model (epoch =  448, loss = 7.9514)\n",
            "Saving model (epoch =  449, loss = 7.9256)\n",
            "Saving model (epoch =  450, loss = 7.9004)\n",
            "Saving model (epoch =  451, loss = 7.8751)\n",
            "Saving model (epoch =  452, loss = 7.8501)\n",
            "Saving model (epoch =  453, loss = 7.8250)\n",
            "Saving model (epoch =  454, loss = 7.8001)\n",
            "Saving model (epoch =  455, loss = 7.7749)\n",
            "Saving model (epoch =  456, loss = 7.7500)\n",
            "Saving model (epoch =  457, loss = 7.7255)\n",
            "Saving model (epoch =  458, loss = 7.7011)\n",
            "Saving model (epoch =  459, loss = 7.6768)\n",
            "Saving model (epoch =  460, loss = 7.6526)\n",
            "Saving model (epoch =  461, loss = 7.6286)\n",
            "Saving model (epoch =  462, loss = 7.6046)\n",
            "Saving model (epoch =  463, loss = 7.5809)\n",
            "Saving model (epoch =  464, loss = 7.5573)\n",
            "Saving model (epoch =  465, loss = 7.5336)\n",
            "Saving model (epoch =  466, loss = 7.5103)\n",
            "Saving model (epoch =  467, loss = 7.4871)\n",
            "Saving model (epoch =  468, loss = 7.4639)\n",
            "Saving model (epoch =  469, loss = 7.4408)\n",
            "Saving model (epoch =  470, loss = 7.4183)\n",
            "Saving model (epoch =  471, loss = 7.3959)\n",
            "Saving model (epoch =  472, loss = 7.3736)\n",
            "Saving model (epoch =  473, loss = 7.3513)\n",
            "Saving model (epoch =  474, loss = 7.3289)\n",
            "Saving model (epoch =  475, loss = 7.3071)\n",
            "Saving model (epoch =  476, loss = 7.2857)\n",
            "Saving model (epoch =  477, loss = 7.2641)\n",
            "Saving model (epoch =  478, loss = 7.2424)\n",
            "Saving model (epoch =  479, loss = 7.2212)\n",
            "Saving model (epoch =  480, loss = 7.1996)\n",
            "Saving model (epoch =  481, loss = 7.1788)\n",
            "Saving model (epoch =  482, loss = 7.1581)\n",
            "Saving model (epoch =  483, loss = 7.1374)\n",
            "Saving model (epoch =  484, loss = 7.1165)\n",
            "Saving model (epoch =  485, loss = 7.0957)\n",
            "Saving model (epoch =  486, loss = 7.0752)\n",
            "Saving model (epoch =  487, loss = 7.0542)\n",
            "Saving model (epoch =  488, loss = 7.0337)\n",
            "Saving model (epoch =  489, loss = 7.0134)\n",
            "Saving model (epoch =  490, loss = 6.9935)\n",
            "Saving model (epoch =  491, loss = 6.9734)\n",
            "Saving model (epoch =  492, loss = 6.9535)\n",
            "Saving model (epoch =  493, loss = 6.9333)\n",
            "Saving model (epoch =  494, loss = 6.9133)\n",
            "Saving model (epoch =  495, loss = 6.8934)\n",
            "Saving model (epoch =  496, loss = 6.8732)\n",
            "Saving model (epoch =  497, loss = 6.8541)\n",
            "Saving model (epoch =  498, loss = 6.8348)\n",
            "Saving model (epoch =  499, loss = 6.8155)\n",
            "Saving model (epoch =  500, loss = 6.7960)\n",
            "Saving model (epoch =  501, loss = 6.7766)\n",
            "Saving model (epoch =  502, loss = 6.7572)\n",
            "Saving model (epoch =  503, loss = 6.7374)\n",
            "Saving model (epoch =  504, loss = 6.7184)\n",
            "Saving model (epoch =  505, loss = 6.6996)\n",
            "Saving model (epoch =  506, loss = 6.6808)\n",
            "Saving model (epoch =  507, loss = 6.6621)\n",
            "Saving model (epoch =  508, loss = 6.6435)\n",
            "Saving model (epoch =  509, loss = 6.6255)\n",
            "Saving model (epoch =  510, loss = 6.6067)\n",
            "Saving model (epoch =  511, loss = 6.5878)\n",
            "Saving model (epoch =  512, loss = 6.5694)\n",
            "Saving model (epoch =  513, loss = 6.5512)\n",
            "Saving model (epoch =  514, loss = 6.5336)\n",
            "Saving model (epoch =  515, loss = 6.5151)\n",
            "Saving model (epoch =  516, loss = 6.4968)\n",
            "Saving model (epoch =  517, loss = 6.4787)\n",
            "Saving model (epoch =  518, loss = 6.4607)\n",
            "Saving model (epoch =  519, loss = 6.4429)\n",
            "Saving model (epoch =  520, loss = 6.4243)\n",
            "Saving model (epoch =  521, loss = 6.4069)\n",
            "Saving model (epoch =  522, loss = 6.3888)\n",
            "Saving model (epoch =  523, loss = 6.3711)\n",
            "Saving model (epoch =  524, loss = 6.3537)\n",
            "Saving model (epoch =  525, loss = 6.3363)\n",
            "Saving model (epoch =  526, loss = 6.3186)\n",
            "Saving model (epoch =  527, loss = 6.3011)\n",
            "Saving model (epoch =  528, loss = 6.2832)\n",
            "Saving model (epoch =  529, loss = 6.2658)\n",
            "Saving model (epoch =  530, loss = 6.2486)\n",
            "Saving model (epoch =  531, loss = 6.2310)\n",
            "Saving model (epoch =  532, loss = 6.2141)\n",
            "Saving model (epoch =  533, loss = 6.1971)\n",
            "Saving model (epoch =  534, loss = 6.1801)\n",
            "Saving model (epoch =  535, loss = 6.1631)\n",
            "Saving model (epoch =  536, loss = 6.1461)\n",
            "Saving model (epoch =  537, loss = 6.1293)\n",
            "Saving model (epoch =  538, loss = 6.1123)\n",
            "Saving model (epoch =  539, loss = 6.0953)\n",
            "Saving model (epoch =  540, loss = 6.0790)\n",
            "Saving model (epoch =  541, loss = 6.0625)\n",
            "Saving model (epoch =  542, loss = 6.0453)\n",
            "Saving model (epoch =  543, loss = 6.0280)\n",
            "Saving model (epoch =  544, loss = 6.0109)\n",
            "Saving model (epoch =  545, loss = 5.9943)\n",
            "Saving model (epoch =  546, loss = 5.9779)\n",
            "Saving model (epoch =  547, loss = 5.9613)\n",
            "Saving model (epoch =  548, loss = 5.9445)\n",
            "Saving model (epoch =  549, loss = 5.9276)\n",
            "Saving model (epoch =  550, loss = 5.9110)\n",
            "Saving model (epoch =  551, loss = 5.8941)\n",
            "Saving model (epoch =  552, loss = 5.8779)\n",
            "Saving model (epoch =  553, loss = 5.8617)\n",
            "Saving model (epoch =  554, loss = 5.8458)\n",
            "Saving model (epoch =  555, loss = 5.8295)\n",
            "Saving model (epoch =  556, loss = 5.8132)\n",
            "Saving model (epoch =  557, loss = 5.7974)\n",
            "Saving model (epoch =  558, loss = 5.7811)\n",
            "Saving model (epoch =  559, loss = 5.7651)\n",
            "Saving model (epoch =  560, loss = 5.7486)\n",
            "Saving model (epoch =  561, loss = 5.7326)\n",
            "Saving model (epoch =  562, loss = 5.7165)\n",
            "Saving model (epoch =  563, loss = 5.7008)\n",
            "Saving model (epoch =  564, loss = 5.6852)\n",
            "Saving model (epoch =  565, loss = 5.6690)\n",
            "Saving model (epoch =  566, loss = 5.6529)\n",
            "Saving model (epoch =  567, loss = 5.6368)\n",
            "Saving model (epoch =  568, loss = 5.6209)\n",
            "Saving model (epoch =  569, loss = 5.6053)\n",
            "Saving model (epoch =  570, loss = 5.5899)\n",
            "Saving model (epoch =  571, loss = 5.5745)\n",
            "Saving model (epoch =  572, loss = 5.5586)\n",
            "Saving model (epoch =  573, loss = 5.5437)\n",
            "Saving model (epoch =  574, loss = 5.5284)\n",
            "Saving model (epoch =  575, loss = 5.5131)\n",
            "Saving model (epoch =  576, loss = 5.4975)\n",
            "Saving model (epoch =  577, loss = 5.4819)\n",
            "Saving model (epoch =  578, loss = 5.4663)\n",
            "Saving model (epoch =  579, loss = 5.4512)\n",
            "Saving model (epoch =  580, loss = 5.4355)\n",
            "Saving model (epoch =  581, loss = 5.4195)\n",
            "Saving model (epoch =  582, loss = 5.4048)\n",
            "Saving model (epoch =  583, loss = 5.3884)\n",
            "Saving model (epoch =  584, loss = 5.3729)\n",
            "Saving model (epoch =  585, loss = 5.3574)\n",
            "Saving model (epoch =  586, loss = 5.3416)\n",
            "Saving model (epoch =  587, loss = 5.3266)\n",
            "Saving model (epoch =  588, loss = 5.3112)\n",
            "Saving model (epoch =  589, loss = 5.2958)\n",
            "Saving model (epoch =  590, loss = 5.2802)\n",
            "Saving model (epoch =  591, loss = 5.2651)\n",
            "Saving model (epoch =  592, loss = 5.2495)\n",
            "Saving model (epoch =  593, loss = 5.2343)\n",
            "Saving model (epoch =  594, loss = 5.2191)\n",
            "Saving model (epoch =  595, loss = 5.2047)\n",
            "Saving model (epoch =  596, loss = 5.1899)\n",
            "Saving model (epoch =  597, loss = 5.1747)\n",
            "Saving model (epoch =  598, loss = 5.1598)\n",
            "Saving model (epoch =  599, loss = 5.1450)\n",
            "Saving model (epoch =  600, loss = 5.1306)\n",
            "Saving model (epoch =  601, loss = 5.1158)\n",
            "Saving model (epoch =  602, loss = 5.1015)\n",
            "Saving model (epoch =  603, loss = 5.0868)\n",
            "Saving model (epoch =  604, loss = 5.0717)\n",
            "Saving model (epoch =  605, loss = 5.0572)\n",
            "Saving model (epoch =  606, loss = 5.0426)\n",
            "Saving model (epoch =  607, loss = 5.0283)\n",
            "Saving model (epoch =  608, loss = 5.0139)\n",
            "Saving model (epoch =  609, loss = 4.9993)\n",
            "Saving model (epoch =  610, loss = 4.9853)\n",
            "Saving model (epoch =  611, loss = 4.9710)\n",
            "Saving model (epoch =  612, loss = 4.9570)\n",
            "Saving model (epoch =  613, loss = 4.9425)\n",
            "Saving model (epoch =  614, loss = 4.9286)\n",
            "Saving model (epoch =  615, loss = 4.9140)\n",
            "Saving model (epoch =  616, loss = 4.9001)\n",
            "Saving model (epoch =  617, loss = 4.8863)\n",
            "Saving model (epoch =  618, loss = 4.8727)\n",
            "Saving model (epoch =  619, loss = 4.8596)\n",
            "Saving model (epoch =  620, loss = 4.8450)\n",
            "Saving model (epoch =  621, loss = 4.8308)\n",
            "Saving model (epoch =  622, loss = 4.8169)\n",
            "Saving model (epoch =  623, loss = 4.8028)\n",
            "Saving model (epoch =  624, loss = 4.7890)\n",
            "Saving model (epoch =  625, loss = 4.7750)\n",
            "Saving model (epoch =  626, loss = 4.7610)\n",
            "Saving model (epoch =  627, loss = 4.7466)\n",
            "Saving model (epoch =  628, loss = 4.7325)\n",
            "Saving model (epoch =  629, loss = 4.7184)\n",
            "Saving model (epoch =  630, loss = 4.7049)\n",
            "Saving model (epoch =  631, loss = 4.6918)\n",
            "Saving model (epoch =  632, loss = 4.6778)\n",
            "Saving model (epoch =  633, loss = 4.6638)\n",
            "Saving model (epoch =  634, loss = 4.6501)\n",
            "Saving model (epoch =  635, loss = 4.6363)\n",
            "Saving model (epoch =  636, loss = 4.6232)\n",
            "Saving model (epoch =  637, loss = 4.6091)\n",
            "Saving model (epoch =  638, loss = 4.5953)\n",
            "Saving model (epoch =  639, loss = 4.5815)\n",
            "Saving model (epoch =  640, loss = 4.5674)\n",
            "Saving model (epoch =  641, loss = 4.5536)\n",
            "Saving model (epoch =  642, loss = 4.5401)\n",
            "Saving model (epoch =  643, loss = 4.5267)\n",
            "Saving model (epoch =  644, loss = 4.5134)\n",
            "Saving model (epoch =  645, loss = 4.4994)\n",
            "Saving model (epoch =  646, loss = 4.4857)\n",
            "Saving model (epoch =  647, loss = 4.4715)\n",
            "Saving model (epoch =  648, loss = 4.4584)\n",
            "Saving model (epoch =  649, loss = 4.4449)\n",
            "Saving model (epoch =  650, loss = 4.4312)\n",
            "Saving model (epoch =  651, loss = 4.4175)\n",
            "Saving model (epoch =  652, loss = 4.4037)\n",
            "Saving model (epoch =  653, loss = 4.3899)\n",
            "Saving model (epoch =  654, loss = 4.3764)\n",
            "Saving model (epoch =  655, loss = 4.3628)\n",
            "Saving model (epoch =  656, loss = 4.3496)\n",
            "Saving model (epoch =  657, loss = 4.3361)\n",
            "Saving model (epoch =  658, loss = 4.3225)\n",
            "Saving model (epoch =  659, loss = 4.3097)\n",
            "Saving model (epoch =  660, loss = 4.2956)\n",
            "Saving model (epoch =  661, loss = 4.2823)\n",
            "Saving model (epoch =  662, loss = 4.2691)\n",
            "Saving model (epoch =  663, loss = 4.2560)\n",
            "Saving model (epoch =  664, loss = 4.2424)\n",
            "Saving model (epoch =  665, loss = 4.2285)\n",
            "Saving model (epoch =  666, loss = 4.2155)\n",
            "Saving model (epoch =  667, loss = 4.2020)\n",
            "Saving model (epoch =  668, loss = 4.1897)\n",
            "Saving model (epoch =  669, loss = 4.1769)\n",
            "Saving model (epoch =  670, loss = 4.1645)\n",
            "Saving model (epoch =  671, loss = 4.1518)\n",
            "Saving model (epoch =  672, loss = 4.1388)\n",
            "Saving model (epoch =  673, loss = 4.1270)\n",
            "Saving model (epoch =  674, loss = 4.1138)\n",
            "Saving model (epoch =  675, loss = 4.1013)\n",
            "Saving model (epoch =  676, loss = 4.0879)\n",
            "Saving model (epoch =  677, loss = 4.0754)\n",
            "Saving model (epoch =  678, loss = 4.0628)\n",
            "Saving model (epoch =  679, loss = 4.0496)\n",
            "Saving model (epoch =  680, loss = 4.0366)\n",
            "Saving model (epoch =  681, loss = 4.0238)\n",
            "Saving model (epoch =  682, loss = 4.0118)\n",
            "Saving model (epoch =  683, loss = 3.9996)\n",
            "Saving model (epoch =  684, loss = 3.9872)\n",
            "Saving model (epoch =  685, loss = 3.9751)\n",
            "Saving model (epoch =  686, loss = 3.9624)\n",
            "Saving model (epoch =  687, loss = 3.9497)\n",
            "Saving model (epoch =  688, loss = 3.9367)\n",
            "Saving model (epoch =  689, loss = 3.9249)\n",
            "Saving model (epoch =  690, loss = 3.9129)\n",
            "Saving model (epoch =  691, loss = 3.9003)\n",
            "Saving model (epoch =  692, loss = 3.8881)\n",
            "Saving model (epoch =  693, loss = 3.8755)\n",
            "Saving model (epoch =  694, loss = 3.8630)\n",
            "Saving model (epoch =  695, loss = 3.8506)\n",
            "Saving model (epoch =  696, loss = 3.8377)\n",
            "Saving model (epoch =  697, loss = 3.8255)\n",
            "Saving model (epoch =  698, loss = 3.8132)\n",
            "Saving model (epoch =  699, loss = 3.8014)\n",
            "Saving model (epoch =  700, loss = 3.7891)\n",
            "Saving model (epoch =  701, loss = 3.7772)\n",
            "Saving model (epoch =  702, loss = 3.7653)\n",
            "Saving model (epoch =  703, loss = 3.7532)\n",
            "Saving model (epoch =  704, loss = 3.7412)\n",
            "Saving model (epoch =  705, loss = 3.7287)\n",
            "Saving model (epoch =  706, loss = 3.7161)\n",
            "Saving model (epoch =  707, loss = 3.7042)\n",
            "Saving model (epoch =  708, loss = 3.6921)\n",
            "Saving model (epoch =  709, loss = 3.6810)\n",
            "Saving model (epoch =  710, loss = 3.6693)\n",
            "Saving model (epoch =  711, loss = 3.6575)\n",
            "Saving model (epoch =  712, loss = 3.6458)\n",
            "Saving model (epoch =  713, loss = 3.6342)\n",
            "Saving model (epoch =  714, loss = 3.6224)\n",
            "Saving model (epoch =  715, loss = 3.6098)\n",
            "Saving model (epoch =  716, loss = 3.5981)\n",
            "Saving model (epoch =  717, loss = 3.5859)\n",
            "Saving model (epoch =  718, loss = 3.5748)\n",
            "Saving model (epoch =  719, loss = 3.5630)\n",
            "Saving model (epoch =  720, loss = 3.5510)\n",
            "Saving model (epoch =  721, loss = 3.5382)\n",
            "Saving model (epoch =  722, loss = 3.5258)\n",
            "Saving model (epoch =  723, loss = 3.5136)\n",
            "Saving model (epoch =  724, loss = 3.5015)\n",
            "Saving model (epoch =  725, loss = 3.4898)\n",
            "Saving model (epoch =  726, loss = 3.4777)\n",
            "Saving model (epoch =  727, loss = 3.4657)\n",
            "Saving model (epoch =  728, loss = 3.4541)\n",
            "Saving model (epoch =  729, loss = 3.4418)\n",
            "Saving model (epoch =  730, loss = 3.4304)\n",
            "Saving model (epoch =  731, loss = 3.4186)\n",
            "Saving model (epoch =  732, loss = 3.4076)\n",
            "Saving model (epoch =  733, loss = 3.3960)\n",
            "Saving model (epoch =  734, loss = 3.3842)\n",
            "Saving model (epoch =  735, loss = 3.3726)\n",
            "Saving model (epoch =  736, loss = 3.3611)\n",
            "Saving model (epoch =  737, loss = 3.3499)\n",
            "Saving model (epoch =  738, loss = 3.3376)\n",
            "Saving model (epoch =  739, loss = 3.3256)\n",
            "Saving model (epoch =  740, loss = 3.3136)\n",
            "Saving model (epoch =  741, loss = 3.3021)\n",
            "Saving model (epoch =  742, loss = 3.2909)\n",
            "Saving model (epoch =  743, loss = 3.2802)\n",
            "Saving model (epoch =  744, loss = 3.2700)\n",
            "Saving model (epoch =  745, loss = 3.2584)\n",
            "Saving model (epoch =  746, loss = 3.2470)\n",
            "Saving model (epoch =  747, loss = 3.2353)\n",
            "Saving model (epoch =  748, loss = 3.2240)\n",
            "Saving model (epoch =  749, loss = 3.2127)\n",
            "Saving model (epoch =  750, loss = 3.2017)\n",
            "Saving model (epoch =  751, loss = 3.1904)\n",
            "Saving model (epoch =  752, loss = 3.1795)\n",
            "Saving model (epoch =  753, loss = 3.1686)\n",
            "Saving model (epoch =  754, loss = 3.1578)\n",
            "Saving model (epoch =  755, loss = 3.1469)\n",
            "Saving model (epoch =  756, loss = 3.1357)\n",
            "Saving model (epoch =  757, loss = 3.1244)\n",
            "Saving model (epoch =  758, loss = 3.1133)\n",
            "Saving model (epoch =  759, loss = 3.1022)\n",
            "Saving model (epoch =  760, loss = 3.0917)\n",
            "Saving model (epoch =  761, loss = 3.0820)\n",
            "Saving model (epoch =  762, loss = 3.0714)\n",
            "Saving model (epoch =  763, loss = 3.0612)\n",
            "Saving model (epoch =  764, loss = 3.0506)\n",
            "Saving model (epoch =  765, loss = 3.0400)\n",
            "Saving model (epoch =  766, loss = 3.0292)\n",
            "Saving model (epoch =  767, loss = 3.0185)\n",
            "Saving model (epoch =  768, loss = 3.0071)\n",
            "Saving model (epoch =  769, loss = 2.9964)\n",
            "Saving model (epoch =  770, loss = 2.9861)\n",
            "Saving model (epoch =  771, loss = 2.9761)\n",
            "Saving model (epoch =  772, loss = 2.9654)\n",
            "Saving model (epoch =  773, loss = 2.9556)\n",
            "Saving model (epoch =  774, loss = 2.9454)\n",
            "Saving model (epoch =  775, loss = 2.9340)\n",
            "Saving model (epoch =  776, loss = 2.9242)\n",
            "Saving model (epoch =  777, loss = 2.9138)\n",
            "Saving model (epoch =  778, loss = 2.9044)\n",
            "Saving model (epoch =  779, loss = 2.8941)\n",
            "Saving model (epoch =  780, loss = 2.8846)\n",
            "Saving model (epoch =  781, loss = 2.8744)\n",
            "Saving model (epoch =  782, loss = 2.8640)\n",
            "Saving model (epoch =  783, loss = 2.8534)\n",
            "Saving model (epoch =  784, loss = 2.8448)\n",
            "Saving model (epoch =  785, loss = 2.8348)\n",
            "Saving model (epoch =  786, loss = 2.8236)\n",
            "Saving model (epoch =  787, loss = 2.8134)\n",
            "Saving model (epoch =  788, loss = 2.8029)\n",
            "Saving model (epoch =  789, loss = 2.7932)\n",
            "Saving model (epoch =  790, loss = 2.7828)\n",
            "Saving model (epoch =  791, loss = 2.7727)\n",
            "Saving model (epoch =  792, loss = 2.7635)\n",
            "Saving model (epoch =  793, loss = 2.7537)\n",
            "Saving model (epoch =  794, loss = 2.7439)\n",
            "Saving model (epoch =  795, loss = 2.7339)\n",
            "Saving model (epoch =  796, loss = 2.7238)\n",
            "Saving model (epoch =  797, loss = 2.7138)\n",
            "Saving model (epoch =  798, loss = 2.7039)\n",
            "Saving model (epoch =  799, loss = 2.6943)\n",
            "Saving model (epoch =  800, loss = 2.6864)\n",
            "Saving model (epoch =  801, loss = 2.6777)\n",
            "Saving model (epoch =  802, loss = 2.6669)\n",
            "Saving model (epoch =  803, loss = 2.6566)\n",
            "Saving model (epoch =  804, loss = 2.6465)\n",
            "Saving model (epoch =  805, loss = 2.6369)\n",
            "Saving model (epoch =  806, loss = 2.6272)\n",
            "Saving model (epoch =  807, loss = 2.6181)\n",
            "Saving model (epoch =  808, loss = 2.6084)\n",
            "Saving model (epoch =  809, loss = 2.5988)\n",
            "Saving model (epoch =  810, loss = 2.5896)\n",
            "Saving model (epoch =  811, loss = 2.5808)\n",
            "Saving model (epoch =  812, loss = 2.5716)\n",
            "Saving model (epoch =  813, loss = 2.5627)\n",
            "Saving model (epoch =  814, loss = 2.5523)\n",
            "Saving model (epoch =  815, loss = 2.5430)\n",
            "Saving model (epoch =  816, loss = 2.5344)\n",
            "Saving model (epoch =  817, loss = 2.5260)\n",
            "Saving model (epoch =  818, loss = 2.5164)\n",
            "Saving model (epoch =  819, loss = 2.5080)\n",
            "Saving model (epoch =  820, loss = 2.4990)\n",
            "Saving model (epoch =  821, loss = 2.4904)\n",
            "Saving model (epoch =  822, loss = 2.4814)\n",
            "Saving model (epoch =  823, loss = 2.4730)\n",
            "Saving model (epoch =  824, loss = 2.4633)\n",
            "Saving model (epoch =  825, loss = 2.4548)\n",
            "Saving model (epoch =  826, loss = 2.4469)\n",
            "Saving model (epoch =  827, loss = 2.4377)\n",
            "Saving model (epoch =  828, loss = 2.4288)\n",
            "Saving model (epoch =  829, loss = 2.4200)\n",
            "Saving model (epoch =  830, loss = 2.4122)\n",
            "Saving model (epoch =  831, loss = 2.4040)\n",
            "Saving model (epoch =  832, loss = 2.3948)\n",
            "Saving model (epoch =  833, loss = 2.3857)\n",
            "Saving model (epoch =  834, loss = 2.3769)\n",
            "Saving model (epoch =  835, loss = 2.3687)\n",
            "Saving model (epoch =  836, loss = 2.3604)\n",
            "Saving model (epoch =  837, loss = 2.3520)\n",
            "Saving model (epoch =  838, loss = 2.3444)\n",
            "Saving model (epoch =  839, loss = 2.3357)\n",
            "Saving model (epoch =  840, loss = 2.3265)\n",
            "Saving model (epoch =  841, loss = 2.3191)\n",
            "Saving model (epoch =  842, loss = 2.3113)\n",
            "Saving model (epoch =  843, loss = 2.3036)\n",
            "Saving model (epoch =  844, loss = 2.2953)\n",
            "Saving model (epoch =  845, loss = 2.2860)\n",
            "Saving model (epoch =  846, loss = 2.2776)\n",
            "Saving model (epoch =  847, loss = 2.2700)\n",
            "Saving model (epoch =  848, loss = 2.2622)\n",
            "Saving model (epoch =  849, loss = 2.2542)\n",
            "Saving model (epoch =  850, loss = 2.2450)\n",
            "Saving model (epoch =  851, loss = 2.2381)\n",
            "Saving model (epoch =  852, loss = 2.2310)\n",
            "Saving model (epoch =  853, loss = 2.2224)\n",
            "Saving model (epoch =  854, loss = 2.2136)\n",
            "Saving model (epoch =  855, loss = 2.2066)\n",
            "Saving model (epoch =  856, loss = 2.1993)\n",
            "Saving model (epoch =  857, loss = 2.1915)\n",
            "Saving model (epoch =  858, loss = 2.1836)\n",
            "Saving model (epoch =  859, loss = 2.1756)\n",
            "Saving model (epoch =  860, loss = 2.1687)\n",
            "Saving model (epoch =  861, loss = 2.1611)\n",
            "Saving model (epoch =  862, loss = 2.1539)\n",
            "Saving model (epoch =  863, loss = 2.1468)\n",
            "Saving model (epoch =  864, loss = 2.1395)\n",
            "Saving model (epoch =  865, loss = 2.1313)\n",
            "Saving model (epoch =  866, loss = 2.1233)\n",
            "Saving model (epoch =  867, loss = 2.1164)\n",
            "Saving model (epoch =  868, loss = 2.1098)\n",
            "Saving model (epoch =  869, loss = 2.1036)\n",
            "Saving model (epoch =  870, loss = 2.0958)\n",
            "Saving model (epoch =  871, loss = 2.0887)\n",
            "Saving model (epoch =  872, loss = 2.0818)\n",
            "Saving model (epoch =  873, loss = 2.0749)\n",
            "Saving model (epoch =  874, loss = 2.0680)\n",
            "Saving model (epoch =  875, loss = 2.0606)\n",
            "Saving model (epoch =  876, loss = 2.0531)\n",
            "Saving model (epoch =  877, loss = 2.0459)\n",
            "Saving model (epoch =  878, loss = 2.0396)\n",
            "Saving model (epoch =  879, loss = 2.0333)\n",
            "Saving model (epoch =  880, loss = 2.0273)\n",
            "Saving model (epoch =  881, loss = 2.0187)\n",
            "Saving model (epoch =  882, loss = 2.0107)\n",
            "Saving model (epoch =  883, loss = 2.0037)\n",
            "Saving model (epoch =  884, loss = 1.9963)\n",
            "Saving model (epoch =  885, loss = 1.9896)\n",
            "Saving model (epoch =  886, loss = 1.9834)\n",
            "Saving model (epoch =  887, loss = 1.9778)\n",
            "Saving model (epoch =  888, loss = 1.9710)\n",
            "Saving model (epoch =  889, loss = 1.9637)\n",
            "Saving model (epoch =  890, loss = 1.9567)\n",
            "Saving model (epoch =  891, loss = 1.9498)\n",
            "Saving model (epoch =  892, loss = 1.9424)\n",
            "Saving model (epoch =  893, loss = 1.9350)\n",
            "Saving model (epoch =  894, loss = 1.9281)\n",
            "Saving model (epoch =  895, loss = 1.9216)\n",
            "Saving model (epoch =  896, loss = 1.9151)\n",
            "Saving model (epoch =  897, loss = 1.9063)\n",
            "Saving model (epoch =  898, loss = 1.8987)\n",
            "Saving model (epoch =  899, loss = 1.8909)\n",
            "Saving model (epoch =  900, loss = 1.8826)\n",
            "Saving model (epoch =  901, loss = 1.8756)\n",
            "Saving model (epoch =  902, loss = 1.8678)\n",
            "Saving model (epoch =  903, loss = 1.8613)\n",
            "Saving model (epoch =  904, loss = 1.8539)\n",
            "Saving model (epoch =  905, loss = 1.8472)\n",
            "Saving model (epoch =  906, loss = 1.8417)\n",
            "Saving model (epoch =  907, loss = 1.8354)\n",
            "Saving model (epoch =  908, loss = 1.8296)\n",
            "Saving model (epoch =  909, loss = 1.8225)\n",
            "Saving model (epoch =  910, loss = 1.8164)\n",
            "Saving model (epoch =  911, loss = 1.8106)\n",
            "Saving model (epoch =  912, loss = 1.8048)\n",
            "Saving model (epoch =  913, loss = 1.7981)\n",
            "Saving model (epoch =  914, loss = 1.7930)\n",
            "Saving model (epoch =  915, loss = 1.7879)\n",
            "Saving model (epoch =  916, loss = 1.7820)\n",
            "Saving model (epoch =  917, loss = 1.7757)\n",
            "Saving model (epoch =  918, loss = 1.7701)\n",
            "Saving model (epoch =  919, loss = 1.7636)\n",
            "Saving model (epoch =  920, loss = 1.7575)\n",
            "Saving model (epoch =  921, loss = 1.7528)\n",
            "Saving model (epoch =  922, loss = 1.7474)\n",
            "Saving model (epoch =  923, loss = 1.7419)\n",
            "Saving model (epoch =  924, loss = 1.7350)\n",
            "Saving model (epoch =  925, loss = 1.7291)\n",
            "Saving model (epoch =  926, loss = 1.7229)\n",
            "Saving model (epoch =  927, loss = 1.7172)\n",
            "Saving model (epoch =  928, loss = 1.7126)\n",
            "Saving model (epoch =  929, loss = 1.7080)\n",
            "Saving model (epoch =  930, loss = 1.7028)\n",
            "Saving model (epoch =  931, loss = 1.6979)\n",
            "Saving model (epoch =  932, loss = 1.6931)\n",
            "Saving model (epoch =  933, loss = 1.6872)\n",
            "Saving model (epoch =  934, loss = 1.6823)\n",
            "Saving model (epoch =  935, loss = 1.6761)\n",
            "Saving model (epoch =  936, loss = 1.6708)\n",
            "Saving model (epoch =  937, loss = 1.6653)\n",
            "Saving model (epoch =  938, loss = 1.6603)\n",
            "Saving model (epoch =  939, loss = 1.6556)\n",
            "Saving model (epoch =  940, loss = 1.6507)\n",
            "Saving model (epoch =  941, loss = 1.6462)\n",
            "Saving model (epoch =  942, loss = 1.6401)\n",
            "Saving model (epoch =  943, loss = 1.6345)\n",
            "Saving model (epoch =  944, loss = 1.6304)\n",
            "Saving model (epoch =  945, loss = 1.6250)\n",
            "Saving model (epoch =  946, loss = 1.6201)\n",
            "Saving model (epoch =  947, loss = 1.6153)\n",
            "Saving model (epoch =  948, loss = 1.6098)\n",
            "Saving model (epoch =  949, loss = 1.6047)\n",
            "Saving model (epoch =  950, loss = 1.6010)\n",
            "Saving model (epoch =  951, loss = 1.5972)\n",
            "Saving model (epoch =  952, loss = 1.5914)\n",
            "Saving model (epoch =  953, loss = 1.5874)\n",
            "Saving model (epoch =  954, loss = 1.5817)\n",
            "Saving model (epoch =  955, loss = 1.5784)\n",
            "Saving model (epoch =  956, loss = 1.5738)\n",
            "Saving model (epoch =  957, loss = 1.5679)\n",
            "Saving model (epoch =  958, loss = 1.5645)\n",
            "Saving model (epoch =  959, loss = 1.5608)\n",
            "Saving model (epoch =  960, loss = 1.5559)\n",
            "Saving model (epoch =  961, loss = 1.5508)\n",
            "Saving model (epoch =  962, loss = 1.5462)\n",
            "Saving model (epoch =  963, loss = 1.5407)\n",
            "Saving model (epoch =  964, loss = 1.5371)\n",
            "Saving model (epoch =  965, loss = 1.5344)\n",
            "Saving model (epoch =  966, loss = 1.5307)\n",
            "Saving model (epoch =  967, loss = 1.5251)\n",
            "Saving model (epoch =  968, loss = 1.5204)\n",
            "Saving model (epoch =  969, loss = 1.5157)\n",
            "Saving model (epoch =  970, loss = 1.5124)\n",
            "Saving model (epoch =  971, loss = 1.5080)\n",
            "Saving model (epoch =  972, loss = 1.5039)\n",
            "Saving model (epoch =  973, loss = 1.5001)\n",
            "Saving model (epoch =  974, loss = 1.4971)\n",
            "Saving model (epoch =  975, loss = 1.4940)\n",
            "Saving model (epoch =  976, loss = 1.4892)\n",
            "Saving model (epoch =  977, loss = 1.4851)\n",
            "Saving model (epoch =  978, loss = 1.4809)\n",
            "Saving model (epoch =  979, loss = 1.4778)\n",
            "Saving model (epoch =  980, loss = 1.4732)\n",
            "Saving model (epoch =  981, loss = 1.4698)\n",
            "Saving model (epoch =  982, loss = 1.4656)\n",
            "Saving model (epoch =  983, loss = 1.4620)\n",
            "Saving model (epoch =  984, loss = 1.4577)\n",
            "Saving model (epoch =  985, loss = 1.4537)\n",
            "Saving model (epoch =  986, loss = 1.4505)\n",
            "Saving model (epoch =  987, loss = 1.4473)\n",
            "Saving model (epoch =  988, loss = 1.4443)\n",
            "Saving model (epoch =  989, loss = 1.4404)\n",
            "Saving model (epoch =  990, loss = 1.4374)\n",
            "Saving model (epoch =  991, loss = 1.4348)\n",
            "Saving model (epoch =  992, loss = 1.4313)\n",
            "Saving model (epoch =  993, loss = 1.4269)\n",
            "Saving model (epoch =  994, loss = 1.4217)\n",
            "Saving model (epoch =  995, loss = 1.4184)\n",
            "Saving model (epoch =  996, loss = 1.4149)\n",
            "Saving model (epoch =  997, loss = 1.4135)\n",
            "Saving model (epoch =  998, loss = 1.4104)\n",
            "Saving model (epoch =  999, loss = 1.4056)\n",
            "Saving model (epoch = 1000, loss = 1.4027)\n",
            "Saving model (epoch = 1001, loss = 1.3997)\n",
            "Saving model (epoch = 1002, loss = 1.3962)\n",
            "Saving model (epoch = 1003, loss = 1.3938)\n",
            "Saving model (epoch = 1004, loss = 1.3902)\n",
            "Saving model (epoch = 1005, loss = 1.3864)\n",
            "Saving model (epoch = 1006, loss = 1.3832)\n",
            "Saving model (epoch = 1007, loss = 1.3801)\n",
            "Saving model (epoch = 1008, loss = 1.3781)\n",
            "Saving model (epoch = 1009, loss = 1.3745)\n",
            "Saving model (epoch = 1010, loss = 1.3715)\n",
            "Saving model (epoch = 1011, loss = 1.3698)\n",
            "Saving model (epoch = 1012, loss = 1.3682)\n",
            "Saving model (epoch = 1013, loss = 1.3634)\n",
            "Saving model (epoch = 1014, loss = 1.3587)\n",
            "Saving model (epoch = 1015, loss = 1.3568)\n",
            "Saving model (epoch = 1016, loss = 1.3542)\n",
            "Saving model (epoch = 1017, loss = 1.3522)\n",
            "Saving model (epoch = 1018, loss = 1.3501)\n",
            "Saving model (epoch = 1019, loss = 1.3462)\n",
            "Saving model (epoch = 1020, loss = 1.3420)\n",
            "Saving model (epoch = 1021, loss = 1.3383)\n",
            "Saving model (epoch = 1022, loss = 1.3355)\n",
            "Saving model (epoch = 1023, loss = 1.3349)\n",
            "Saving model (epoch = 1024, loss = 1.3321)\n",
            "Saving model (epoch = 1025, loss = 1.3286)\n",
            "Saving model (epoch = 1026, loss = 1.3254)\n",
            "Saving model (epoch = 1027, loss = 1.3224)\n",
            "Saving model (epoch = 1028, loss = 1.3184)\n",
            "Saving model (epoch = 1029, loss = 1.3159)\n",
            "Saving model (epoch = 1030, loss = 1.3146)\n",
            "Saving model (epoch = 1031, loss = 1.3128)\n",
            "Saving model (epoch = 1032, loss = 1.3086)\n",
            "Saving model (epoch = 1033, loss = 1.3053)\n",
            "Saving model (epoch = 1034, loss = 1.3042)\n",
            "Saving model (epoch = 1035, loss = 1.3030)\n",
            "Saving model (epoch = 1036, loss = 1.3010)\n",
            "Saving model (epoch = 1037, loss = 1.2975)\n",
            "Saving model (epoch = 1038, loss = 1.2954)\n",
            "Saving model (epoch = 1039, loss = 1.2931)\n",
            "Saving model (epoch = 1040, loss = 1.2908)\n",
            "Saving model (epoch = 1041, loss = 1.2883)\n",
            "Saving model (epoch = 1042, loss = 1.2864)\n",
            "Saving model (epoch = 1043, loss = 1.2831)\n",
            "Saving model (epoch = 1044, loss = 1.2803)\n",
            "Saving model (epoch = 1045, loss = 1.2786)\n",
            "Saving model (epoch = 1046, loss = 1.2754)\n",
            "Saving model (epoch = 1047, loss = 1.2733)\n",
            "Saving model (epoch = 1048, loss = 1.2705)\n",
            "Saving model (epoch = 1049, loss = 1.2687)\n",
            "Saving model (epoch = 1050, loss = 1.2673)\n",
            "Saving model (epoch = 1051, loss = 1.2633)\n",
            "Saving model (epoch = 1052, loss = 1.2607)\n",
            "Saving model (epoch = 1053, loss = 1.2588)\n",
            "Saving model (epoch = 1054, loss = 1.2577)\n",
            "Saving model (epoch = 1055, loss = 1.2553)\n",
            "Saving model (epoch = 1056, loss = 1.2523)\n",
            "Saving model (epoch = 1057, loss = 1.2506)\n",
            "Saving model (epoch = 1058, loss = 1.2500)\n",
            "Saving model (epoch = 1059, loss = 1.2479)\n",
            "Saving model (epoch = 1060, loss = 1.2453)\n",
            "Saving model (epoch = 1061, loss = 1.2432)\n",
            "Saving model (epoch = 1062, loss = 1.2407)\n",
            "Saving model (epoch = 1063, loss = 1.2393)\n",
            "Saving model (epoch = 1064, loss = 1.2368)\n",
            "Saving model (epoch = 1065, loss = 1.2347)\n",
            "Saving model (epoch = 1066, loss = 1.2321)\n",
            "Saving model (epoch = 1067, loss = 1.2300)\n",
            "Saving model (epoch = 1068, loss = 1.2275)\n",
            "Saving model (epoch = 1069, loss = 1.2259)\n",
            "Saving model (epoch = 1070, loss = 1.2250)\n",
            "Saving model (epoch = 1071, loss = 1.2230)\n",
            "Saving model (epoch = 1072, loss = 1.2205)\n",
            "Saving model (epoch = 1073, loss = 1.2183)\n",
            "Saving model (epoch = 1074, loss = 1.2160)\n",
            "Saving model (epoch = 1075, loss = 1.2133)\n",
            "Saving model (epoch = 1076, loss = 1.2114)\n",
            "Saving model (epoch = 1077, loss = 1.2113)\n",
            "Saving model (epoch = 1078, loss = 1.2112)\n",
            "Saving model (epoch = 1079, loss = 1.2077)\n",
            "Saving model (epoch = 1080, loss = 1.2063)\n",
            "Saving model (epoch = 1081, loss = 1.2040)\n",
            "Saving model (epoch = 1082, loss = 1.2030)\n",
            "Saving model (epoch = 1083, loss = 1.2020)\n",
            "Saving model (epoch = 1084, loss = 1.1979)\n",
            "Saving model (epoch = 1085, loss = 1.1970)\n",
            "Saving model (epoch = 1086, loss = 1.1947)\n",
            "Saving model (epoch = 1087, loss = 1.1944)\n",
            "Saving model (epoch = 1088, loss = 1.1938)\n",
            "Saving model (epoch = 1089, loss = 1.1915)\n",
            "Saving model (epoch = 1090, loss = 1.1892)\n",
            "Saving model (epoch = 1091, loss = 1.1864)\n",
            "Saving model (epoch = 1092, loss = 1.1854)\n",
            "Saving model (epoch = 1093, loss = 1.1847)\n",
            "Saving model (epoch = 1094, loss = 1.1835)\n",
            "Saving model (epoch = 1095, loss = 1.1818)\n",
            "Saving model (epoch = 1096, loss = 1.1813)\n",
            "Saving model (epoch = 1097, loss = 1.1794)\n",
            "Saving model (epoch = 1098, loss = 1.1773)\n",
            "Saving model (epoch = 1099, loss = 1.1763)\n",
            "Saving model (epoch = 1100, loss = 1.1755)\n",
            "Saving model (epoch = 1101, loss = 1.1751)\n",
            "Saving model (epoch = 1102, loss = 1.1730)\n",
            "Saving model (epoch = 1103, loss = 1.1705)\n",
            "Saving model (epoch = 1104, loss = 1.1683)\n",
            "Saving model (epoch = 1106, loss = 1.1664)\n",
            "Saving model (epoch = 1107, loss = 1.1650)\n",
            "Saving model (epoch = 1108, loss = 1.1642)\n",
            "Saving model (epoch = 1109, loss = 1.1630)\n",
            "Saving model (epoch = 1110, loss = 1.1623)\n",
            "Saving model (epoch = 1111, loss = 1.1600)\n",
            "Saving model (epoch = 1112, loss = 1.1579)\n",
            "Saving model (epoch = 1113, loss = 1.1561)\n",
            "Saving model (epoch = 1114, loss = 1.1548)\n",
            "Saving model (epoch = 1115, loss = 1.1539)\n",
            "Saving model (epoch = 1116, loss = 1.1531)\n",
            "Saving model (epoch = 1117, loss = 1.1518)\n",
            "Saving model (epoch = 1118, loss = 1.1507)\n",
            "Saving model (epoch = 1119, loss = 1.1499)\n",
            "Saving model (epoch = 1120, loss = 1.1491)\n",
            "Saving model (epoch = 1121, loss = 1.1473)\n",
            "Saving model (epoch = 1122, loss = 1.1459)\n",
            "Saving model (epoch = 1123, loss = 1.1443)\n",
            "Saving model (epoch = 1124, loss = 1.1423)\n",
            "Saving model (epoch = 1125, loss = 1.1413)\n",
            "Saving model (epoch = 1126, loss = 1.1404)\n",
            "Saving model (epoch = 1127, loss = 1.1396)\n",
            "Saving model (epoch = 1128, loss = 1.1373)\n",
            "Saving model (epoch = 1129, loss = 1.1357)\n",
            "Saving model (epoch = 1130, loss = 1.1355)\n",
            "Saving model (epoch = 1131, loss = 1.1346)\n",
            "Saving model (epoch = 1132, loss = 1.1345)\n",
            "Saving model (epoch = 1133, loss = 1.1326)\n",
            "Saving model (epoch = 1134, loss = 1.1320)\n",
            "Saving model (epoch = 1135, loss = 1.1305)\n",
            "Saving model (epoch = 1136, loss = 1.1292)\n",
            "Saving model (epoch = 1137, loss = 1.1281)\n",
            "Saving model (epoch = 1139, loss = 1.1270)\n",
            "Saving model (epoch = 1140, loss = 1.1250)\n",
            "Saving model (epoch = 1141, loss = 1.1241)\n",
            "Saving model (epoch = 1142, loss = 1.1236)\n",
            "Saving model (epoch = 1143, loss = 1.1229)\n",
            "Saving model (epoch = 1144, loss = 1.1216)\n",
            "Saving model (epoch = 1145, loss = 1.1190)\n",
            "Saving model (epoch = 1146, loss = 1.1174)\n",
            "Saving model (epoch = 1147, loss = 1.1169)\n",
            "Saving model (epoch = 1148, loss = 1.1167)\n",
            "Saving model (epoch = 1149, loss = 1.1152)\n",
            "Saving model (epoch = 1150, loss = 1.1148)\n",
            "Saving model (epoch = 1151, loss = 1.1143)\n",
            "Saving model (epoch = 1152, loss = 1.1121)\n",
            "Saving model (epoch = 1154, loss = 1.1101)\n",
            "Saving model (epoch = 1155, loss = 1.1081)\n",
            "Saving model (epoch = 1157, loss = 1.1073)\n",
            "Saving model (epoch = 1158, loss = 1.1063)\n",
            "Saving model (epoch = 1159, loss = 1.1054)\n",
            "Saving model (epoch = 1161, loss = 1.1029)\n",
            "Saving model (epoch = 1162, loss = 1.1023)\n",
            "Saving model (epoch = 1164, loss = 1.1013)\n",
            "Saving model (epoch = 1165, loss = 1.1002)\n",
            "Saving model (epoch = 1166, loss = 1.0986)\n",
            "Saving model (epoch = 1167, loss = 1.0986)\n",
            "Saving model (epoch = 1168, loss = 1.0973)\n",
            "Saving model (epoch = 1169, loss = 1.0968)\n",
            "Saving model (epoch = 1170, loss = 1.0954)\n",
            "Saving model (epoch = 1171, loss = 1.0941)\n",
            "Saving model (epoch = 1172, loss = 1.0930)\n",
            "Saving model (epoch = 1175, loss = 1.0907)\n",
            "Saving model (epoch = 1177, loss = 1.0900)\n",
            "Saving model (epoch = 1178, loss = 1.0880)\n",
            "Saving model (epoch = 1179, loss = 1.0873)\n",
            "Saving model (epoch = 1180, loss = 1.0869)\n",
            "Saving model (epoch = 1181, loss = 1.0856)\n",
            "Saving model (epoch = 1182, loss = 1.0830)\n",
            "Saving model (epoch = 1186, loss = 1.0822)\n",
            "Saving model (epoch = 1187, loss = 1.0811)\n",
            "Saving model (epoch = 1188, loss = 1.0800)\n",
            "Saving model (epoch = 1189, loss = 1.0791)\n",
            "Saving model (epoch = 1190, loss = 1.0790)\n",
            "Saving model (epoch = 1191, loss = 1.0775)\n",
            "Saving model (epoch = 1192, loss = 1.0767)\n",
            "Saving model (epoch = 1193, loss = 1.0758)\n",
            "Saving model (epoch = 1195, loss = 1.0755)\n",
            "Saving model (epoch = 1196, loss = 1.0728)\n",
            "Saving model (epoch = 1197, loss = 1.0719)\n",
            "Saving model (epoch = 1198, loss = 1.0719)\n",
            "Saving model (epoch = 1199, loss = 1.0709)\n",
            "Saving model (epoch = 1200, loss = 1.0697)\n",
            "Saving model (epoch = 1201, loss = 1.0687)\n",
            "Saving model (epoch = 1202, loss = 1.0680)\n",
            "Saving model (epoch = 1203, loss = 1.0671)\n",
            "Saving model (epoch = 1204, loss = 1.0668)\n",
            "Saving model (epoch = 1205, loss = 1.0657)\n",
            "Saving model (epoch = 1207, loss = 1.0646)\n",
            "Saving model (epoch = 1209, loss = 1.0631)\n",
            "Saving model (epoch = 1210, loss = 1.0624)\n",
            "Saving model (epoch = 1211, loss = 1.0617)\n",
            "Saving model (epoch = 1212, loss = 1.0614)\n",
            "Saving model (epoch = 1213, loss = 1.0600)\n",
            "Saving model (epoch = 1214, loss = 1.0598)\n",
            "Saving model (epoch = 1215, loss = 1.0577)\n",
            "Saving model (epoch = 1216, loss = 1.0568)\n",
            "Saving model (epoch = 1217, loss = 1.0555)\n",
            "Saving model (epoch = 1220, loss = 1.0551)\n",
            "Saving model (epoch = 1222, loss = 1.0550)\n",
            "Saving model (epoch = 1223, loss = 1.0529)\n",
            "Saving model (epoch = 1225, loss = 1.0510)\n",
            "Saving model (epoch = 1226, loss = 1.0501)\n",
            "Saving model (epoch = 1227, loss = 1.0499)\n",
            "Saving model (epoch = 1230, loss = 1.0487)\n",
            "Saving model (epoch = 1231, loss = 1.0470)\n",
            "Saving model (epoch = 1232, loss = 1.0456)\n",
            "Saving model (epoch = 1234, loss = 1.0453)\n",
            "Saving model (epoch = 1235, loss = 1.0444)\n",
            "Saving model (epoch = 1236, loss = 1.0437)\n",
            "Saving model (epoch = 1238, loss = 1.0434)\n",
            "Saving model (epoch = 1239, loss = 1.0411)\n",
            "Saving model (epoch = 1240, loss = 1.0408)\n",
            "Saving model (epoch = 1242, loss = 1.0394)\n",
            "Saving model (epoch = 1243, loss = 1.0390)\n",
            "Saving model (epoch = 1244, loss = 1.0383)\n",
            "Saving model (epoch = 1245, loss = 1.0377)\n",
            "Saving model (epoch = 1248, loss = 1.0353)\n",
            "Saving model (epoch = 1249, loss = 1.0339)\n",
            "Saving model (epoch = 1250, loss = 1.0330)\n",
            "Saving model (epoch = 1254, loss = 1.0324)\n",
            "Saving model (epoch = 1255, loss = 1.0318)\n",
            "Saving model (epoch = 1256, loss = 1.0311)\n",
            "Saving model (epoch = 1257, loss = 1.0309)\n",
            "Saving model (epoch = 1258, loss = 1.0299)\n",
            "Saving model (epoch = 1260, loss = 1.0291)\n",
            "Saving model (epoch = 1262, loss = 1.0285)\n",
            "Saving model (epoch = 1263, loss = 1.0274)\n",
            "Saving model (epoch = 1264, loss = 1.0264)\n",
            "Saving model (epoch = 1265, loss = 1.0251)\n",
            "Saving model (epoch = 1266, loss = 1.0246)\n",
            "Saving model (epoch = 1270, loss = 1.0241)\n",
            "Saving model (epoch = 1271, loss = 1.0220)\n",
            "Saving model (epoch = 1274, loss = 1.0217)\n",
            "Saving model (epoch = 1277, loss = 1.0191)\n",
            "Saving model (epoch = 1279, loss = 1.0188)\n",
            "Saving model (epoch = 1280, loss = 1.0187)\n",
            "Saving model (epoch = 1282, loss = 1.0187)\n",
            "Saving model (epoch = 1283, loss = 1.0185)\n",
            "Saving model (epoch = 1284, loss = 1.0175)\n",
            "Saving model (epoch = 1288, loss = 1.0147)\n",
            "Saving model (epoch = 1290, loss = 1.0143)\n",
            "Saving model (epoch = 1292, loss = 1.0141)\n",
            "Saving model (epoch = 1293, loss = 1.0135)\n",
            "Saving model (epoch = 1294, loss = 1.0124)\n",
            "Saving model (epoch = 1295, loss = 1.0121)\n",
            "Saving model (epoch = 1299, loss = 1.0110)\n",
            "Saving model (epoch = 1301, loss = 1.0092)\n",
            "Saving model (epoch = 1302, loss = 1.0082)\n",
            "Saving model (epoch = 1304, loss = 1.0082)\n",
            "Saving model (epoch = 1306, loss = 1.0068)\n",
            "Saving model (epoch = 1307, loss = 1.0059)\n",
            "Saving model (epoch = 1310, loss = 1.0044)\n",
            "Saving model (epoch = 1313, loss = 1.0037)\n",
            "Saving model (epoch = 1315, loss = 1.0026)\n",
            "Saving model (epoch = 1317, loss = 1.0022)\n",
            "Saving model (epoch = 1318, loss = 1.0008)\n",
            "Saving model (epoch = 1324, loss = 0.9999)\n",
            "Saving model (epoch = 1325, loss = 0.9993)\n",
            "Saving model (epoch = 1326, loss = 0.9978)\n",
            "Saving model (epoch = 1327, loss = 0.9972)\n",
            "Saving model (epoch = 1329, loss = 0.9972)\n",
            "Saving model (epoch = 1330, loss = 0.9959)\n",
            "Saving model (epoch = 1331, loss = 0.9955)\n",
            "Saving model (epoch = 1334, loss = 0.9947)\n",
            "Saving model (epoch = 1335, loss = 0.9943)\n",
            "Saving model (epoch = 1336, loss = 0.9938)\n",
            "Saving model (epoch = 1337, loss = 0.9937)\n",
            "Saving model (epoch = 1339, loss = 0.9928)\n",
            "Saving model (epoch = 1342, loss = 0.9916)\n",
            "Saving model (epoch = 1343, loss = 0.9916)\n",
            "Saving model (epoch = 1344, loss = 0.9900)\n",
            "Saving model (epoch = 1346, loss = 0.9896)\n",
            "Saving model (epoch = 1347, loss = 0.9886)\n",
            "Saving model (epoch = 1354, loss = 0.9878)\n",
            "Saving model (epoch = 1355, loss = 0.9866)\n",
            "Saving model (epoch = 1360, loss = 0.9866)\n",
            "Saving model (epoch = 1363, loss = 0.9852)\n",
            "Saving model (epoch = 1365, loss = 0.9848)\n",
            "Saving model (epoch = 1366, loss = 0.9833)\n",
            "Saving model (epoch = 1368, loss = 0.9830)\n",
            "Saving model (epoch = 1369, loss = 0.9824)\n",
            "Saving model (epoch = 1372, loss = 0.9824)\n",
            "Saving model (epoch = 1373, loss = 0.9824)\n",
            "Saving model (epoch = 1374, loss = 0.9815)\n",
            "Saving model (epoch = 1378, loss = 0.9805)\n",
            "Saving model (epoch = 1379, loss = 0.9800)\n",
            "Saving model (epoch = 1381, loss = 0.9789)\n",
            "Saving model (epoch = 1383, loss = 0.9786)\n",
            "Saving model (epoch = 1384, loss = 0.9778)\n",
            "Saving model (epoch = 1390, loss = 0.9775)\n",
            "Saving model (epoch = 1392, loss = 0.9772)\n",
            "Saving model (epoch = 1393, loss = 0.9763)\n",
            "Saving model (epoch = 1394, loss = 0.9763)\n",
            "Saving model (epoch = 1397, loss = 0.9748)\n",
            "Saving model (epoch = 1398, loss = 0.9744)\n",
            "Saving model (epoch = 1401, loss = 0.9741)\n",
            "Saving model (epoch = 1402, loss = 0.9720)\n",
            "Saving model (epoch = 1409, loss = 0.9718)\n",
            "Saving model (epoch = 1410, loss = 0.9713)\n",
            "Saving model (epoch = 1412, loss = 0.9702)\n",
            "Saving model (epoch = 1413, loss = 0.9691)\n",
            "Saving model (epoch = 1417, loss = 0.9686)\n",
            "Saving model (epoch = 1420, loss = 0.9682)\n",
            "Saving model (epoch = 1421, loss = 0.9679)\n",
            "Saving model (epoch = 1425, loss = 0.9671)\n",
            "Saving model (epoch = 1426, loss = 0.9666)\n",
            "Saving model (epoch = 1427, loss = 0.9657)\n",
            "Saving model (epoch = 1433, loss = 0.9652)\n",
            "Saving model (epoch = 1435, loss = 0.9635)\n",
            "Saving model (epoch = 1441, loss = 0.9620)\n",
            "Saving model (epoch = 1445, loss = 0.9616)\n",
            "Saving model (epoch = 1446, loss = 0.9615)\n",
            "Saving model (epoch = 1450, loss = 0.9603)\n",
            "Saving model (epoch = 1451, loss = 0.9594)\n",
            "Saving model (epoch = 1455, loss = 0.9586)\n",
            "Saving model (epoch = 1461, loss = 0.9575)\n",
            "Saving model (epoch = 1462, loss = 0.9573)\n",
            "Saving model (epoch = 1463, loss = 0.9561)\n",
            "Saving model (epoch = 1466, loss = 0.9559)\n",
            "Saving model (epoch = 1467, loss = 0.9552)\n",
            "Saving model (epoch = 1471, loss = 0.9546)\n",
            "Saving model (epoch = 1472, loss = 0.9545)\n",
            "Saving model (epoch = 1476, loss = 0.9540)\n",
            "Saving model (epoch = 1483, loss = 0.9532)\n",
            "Saving model (epoch = 1484, loss = 0.9523)\n",
            "Saving model (epoch = 1489, loss = 0.9509)\n",
            "Saving model (epoch = 1492, loss = 0.9500)\n",
            "Saving model (epoch = 1500, loss = 0.9495)\n",
            "Saving model (epoch = 1502, loss = 0.9495)\n",
            "Saving model (epoch = 1503, loss = 0.9493)\n",
            "Saving model (epoch = 1507, loss = 0.9486)\n",
            "Saving model (epoch = 1508, loss = 0.9477)\n",
            "Saving model (epoch = 1513, loss = 0.9467)\n",
            "Saving model (epoch = 1522, loss = 0.9445)\n",
            "Saving model (epoch = 1537, loss = 0.9435)\n",
            "Saving model (epoch = 1538, loss = 0.9425)\n",
            "Saving model (epoch = 1540, loss = 0.9423)\n",
            "Saving model (epoch = 1541, loss = 0.9422)\n",
            "Saving model (epoch = 1551, loss = 0.9420)\n",
            "Saving model (epoch = 1554, loss = 0.9416)\n",
            "Saving model (epoch = 1555, loss = 0.9410)\n",
            "Saving model (epoch = 1558, loss = 0.9409)\n",
            "Saving model (epoch = 1562, loss = 0.9409)\n",
            "Saving model (epoch = 1565, loss = 0.9397)\n",
            "Saving model (epoch = 1566, loss = 0.9392)\n",
            "Saving model (epoch = 1569, loss = 0.9390)\n",
            "Saving model (epoch = 1571, loss = 0.9385)\n",
            "Saving model (epoch = 1582, loss = 0.9377)\n",
            "Saving model (epoch = 1583, loss = 0.9372)\n",
            "Saving model (epoch = 1594, loss = 0.9356)\n",
            "Saving model (epoch = 1607, loss = 0.9345)\n",
            "Saving model (epoch = 1621, loss = 0.9340)\n",
            "Saving model (epoch = 1624, loss = 0.9340)\n",
            "Saving model (epoch = 1629, loss = 0.9325)\n",
            "Saving model (epoch = 1642, loss = 0.9324)\n",
            "Saving model (epoch = 1645, loss = 0.9315)\n",
            "Saving model (epoch = 1646, loss = 0.9311)\n",
            "Saving model (epoch = 1649, loss = 0.9300)\n",
            "Saving model (epoch = 1656, loss = 0.9295)\n",
            "Saving model (epoch = 1660, loss = 0.9294)\n",
            "Saving model (epoch = 1673, loss = 0.9291)\n",
            "Saving model (epoch = 1674, loss = 0.9276)\n",
            "Saving model (epoch = 1675, loss = 0.9266)\n",
            "Saving model (epoch = 1690, loss = 0.9254)\n",
            "Saving model (epoch = 1698, loss = 0.9248)\n",
            "Saving model (epoch = 1699, loss = 0.9226)\n",
            "Saving model (epoch = 1731, loss = 0.9221)\n",
            "Saving model (epoch = 1733, loss = 0.9221)\n",
            "Saving model (epoch = 1738, loss = 0.9213)\n",
            "Saving model (epoch = 1746, loss = 0.9212)\n",
            "Saving model (epoch = 1749, loss = 0.9202)\n",
            "Saving model (epoch = 1765, loss = 0.9201)\n",
            "Saving model (epoch = 1768, loss = 0.9193)\n",
            "Saving model (epoch = 1779, loss = 0.9189)\n",
            "Saving model (epoch = 1781, loss = 0.9183)\n",
            "Saving model (epoch = 1783, loss = 0.9183)\n",
            "Saving model (epoch = 1788, loss = 0.9170)\n",
            "Saving model (epoch = 1790, loss = 0.9162)\n",
            "Saving model (epoch = 1801, loss = 0.9157)\n",
            "Saving model (epoch = 1821, loss = 0.9151)\n",
            "Saving model (epoch = 1823, loss = 0.9150)\n",
            "Saving model (epoch = 1826, loss = 0.9147)\n",
            "Saving model (epoch = 1838, loss = 0.9125)\n",
            "Saving model (epoch = 1849, loss = 0.9121)\n",
            "Saving model (epoch = 1853, loss = 0.9113)\n",
            "Saving model (epoch = 1871, loss = 0.9096)\n",
            "Saving model (epoch = 1883, loss = 0.9095)\n",
            "Saving model (epoch = 1888, loss = 0.9094)\n",
            "Saving model (epoch = 1898, loss = 0.9086)\n",
            "Saving model (epoch = 1904, loss = 0.9081)\n",
            "Saving model (epoch = 1915, loss = 0.9074)\n",
            "Saving model (epoch = 1918, loss = 0.9057)\n",
            "Saving model (epoch = 1955, loss = 0.9052)\n",
            "Saving model (epoch = 1956, loss = 0.9043)\n",
            "Saving model (epoch = 1959, loss = 0.9041)\n",
            "Saving model (epoch = 1987, loss = 0.9040)\n",
            "Saving model (epoch = 1991, loss = 0.9040)\n",
            "Saving model (epoch = 1996, loss = 0.9031)\n",
            "Saving model (epoch = 2008, loss = 0.9030)\n",
            "Saving model (epoch = 2021, loss = 0.9025)\n",
            "Saving model (epoch = 2036, loss = 0.9022)\n",
            "Saving model (epoch = 2047, loss = 0.9022)\n",
            "Saving model (epoch = 2049, loss = 0.9020)\n",
            "Saving model (epoch = 2050, loss = 0.9018)\n",
            "Saving model (epoch = 2058, loss = 0.9014)\n",
            "Saving model (epoch = 2059, loss = 0.9006)\n",
            "Saving model (epoch = 2065, loss = 0.9000)\n",
            "Saving model (epoch = 2073, loss = 0.8992)\n",
            "Saving model (epoch = 2085, loss = 0.8988)\n",
            "Saving model (epoch = 2127, loss = 0.8985)\n",
            "Saving model (epoch = 2132, loss = 0.8980)\n",
            "Saving model (epoch = 2137, loss = 0.8977)\n",
            "Saving model (epoch = 2145, loss = 0.8968)\n",
            "Saving model (epoch = 2165, loss = 0.8959)\n",
            "Saving model (epoch = 2217, loss = 0.8955)\n",
            "Saving model (epoch = 2268, loss = 0.8951)\n",
            "Saving model (epoch = 2323, loss = 0.8939)\n",
            "Saving model (epoch = 2408, loss = 0.8937)\n",
            "Saving model (epoch = 2409, loss = 0.8932)\n",
            "Saving model (epoch = 2445, loss = 0.8928)\n",
            "Saving model (epoch = 2512, loss = 0.8928)\n",
            "Saving model (epoch = 2526, loss = 0.8927)\n",
            "Saving model (epoch = 2561, loss = 0.8916)\n",
            "Saving model (epoch = 2717, loss = 0.8911)\n",
            "Saving model (epoch = 2749, loss = 0.8907)\n",
            "Saving model (epoch = 2781, loss = 0.8904)\n",
            "Saving model (epoch = 2906, loss = 0.8901)\n",
            "Saving model (epoch = 2920, loss = 0.8899)\n",
            "Saving model (epoch = 2928, loss = 0.8897)\n",
            "Saving model (epoch = 2956, loss = 0.8894)\n",
            "Saving model (epoch = 3057, loss = 0.8891)\n",
            "Saving model (epoch = 3090, loss = 0.8888)\n",
            "Saving model (epoch = 3171, loss = 0.8887)\n",
            "Saving model (epoch = 3259, loss = 0.8878)\n",
            "Saving model (epoch = 3794, loss = 0.8873)\n",
            "Saving model (epoch = 5635, loss = 0.8872)\n",
            "Finished training after 6000 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "3713b913-0233-42ba-d912-e9c06225ac87"
      },
      "source": [
        "plot_learning_curve(model_loss_record, title='deep model')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "4a977938-424b-4c7a-859e-a853ff2787bc"
      },
      "source": [
        "del model\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAFNCAYAAACE8D3EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeXiU5dWH7zPZN0KAsBMQUFwQUdHiUgEVBRXRqLhVXFCq1lpU6lot1Vqt1Vo3UFxRwY0GxQ1F1s8qKiJGRFBAiMRAWLJB9pnn++PMmCEmMAmZLOTc15UrM2/e5ZlRf57znE2ccxiGYRih4WnqBRiGYbQkTDQNwzDqgImmYRhGHTDRNAzDqAMmmoZhGHXARNMwDKMOmGgaISEivUTEiUhkEzx7vYic3NjPbWyqf8ci8r6IXFqP+6SJyA4RiWj4VRomms0IEblARD4TkZ0ikut/fa2ISFOvbXf4/wMN/PhEpCTo/cV1vNcLIvL3cK11bxGRy0TE6/9shSKyXETOCMeznHMjnXPTQljTLv9Tcc5lOecSnXPecKyrtWOi2UwQkZuAR4B/AZ2BTsDVwHFAdC3XNAtLwv8faKJzLhHIAkYFHZseOK8prNQw8an/s7YFngVeF5GU6iftQ5/XCMJEsxkgIsnA3cC1zrmZzrkip3zlnLvYOVfmP+8FEZkiIu+JyE5gmIgcJCILRSRfRL4VkTOD7rtQRK4Men+ZiHwc9N6JyNUi8oP/+icCVq2IRIjIgyKyVUTWAafX43MNFZGNInKLiGwCnq++hqB19BWR8cDFwM1+S+7toNMGikimiBSIyGsiElvD82L8n6N/0LFUv+Xbsdq5fUVkkf9+W0Xktbp+PuecD3gOiAP6iMgkEZkpIi+LSCFwmYgki8izIpIjItki8vfA/+z29B3X8M/vKhH5TkSKRGSliBwhIi8BacDb/u/s5hrc/K4iMltEtovIGhG5Kuiek0TkdRF50X/fb0VkUF2/i9aEiWbz4BggBngrhHMvAu4FkoDPgLeBD4GOwB+B6SLSrw7PPgM4ChgAjAFO9R+/yv+3w4FBwLl1uGcwnYF2QE9g/O5OdM5NBaYDD/it1FFBfx4DjAD286/1shquLwMygAurXbfIOZdb7fR70O8tBegOPBb6R1L8onQlsAP4wX94NDATtUKnAy8AlUBf9Ls8xX8N1OE7FpHzgEnAWKANcCawzTl3Cbta9w/UcPmrwEagq/8Z/xCRE4P+fqb/nLbAbODxEL+CVomJZvOgA7DVOVcZOCAin/itphIROSHo3Lecc//zWzkDgUTgfudcuXNuPvAOu4rGnrjfOZfvnMsCFvjvCSo2/3HO/eSc2w7cV8/P5gP+6pwrc86V1PMeAI865372r+XtoHVWZwZwQdD7i/zHqlOBCnlX51ypc+7jGs6pjcEikg9sQr/rs51zBf6/feqce9P/z6cNcBowwTm30y/cDwetry7f8ZXo/0y+8Hsha5xzG/a0UBHpgW7x3OL/nMuBZ1DxDfCxc+49/x7oS8BhIX4PrRITzebBNqBD8B6Yc+5Y51xb/9+C/zn9FPS6K/CT/z/QABuAbnV49qag18WoCP9y72r3rQ9bnHOl9bw2mNrWWZ0FQLyI/EZEeqHiOquG824GBPjc75JeUYe1LHHOtXXOdXDODXbOfRT0t+DvrCcQBeT4/weYDzyFegVQt++4B7C2DmsM0BXY7pwrqvac4H9Hqn+3sbYfWzv2xTQPPgXKUNfuv3s4N7gt1c9ADxHxBAlnGvC9//VOID7o/M51WFMO+h9qgLQ6XBtM9TZau6xJRKqvaa/abjnnvCLyOmoBbgbeqSYYgfM2oe4xInI88JGILHbOrdmb57Pr+n9C/7l2CPYigqjLd/wT0CeEZ1bnZ6CdiCQFfQ9pQPZurjF2g1mazQDnXD7wN2CyiJwrIkki4hGRgUDCbi79DLUMbhaRKBEZCoxC96cAlgPpIhIvIn2BcXVY1uvA9SLS3R8ZvrWOH6s2vgYOEZGB/mDOpGp/3wz03stnzADOR4NKNbnmiMh5ItLd/zYPFR5fTefWF+dcDrpv+pCItPH/M+0jIkP8p9TlO34GmCgiR4rSV0R6+v9W63fmnPsJ+AS4T0RiRWQA+u/Byw3wEVslJprNBP8G/o2o27jZ//MUcAv6L31N15SjIjkS2ApMBsY651b5T3kYKPffaxoamAiVp4EPUJFbhgZY9hrn3PdopsBHaPCk+l7is8DBfnf2zXo+4zPUou0KvB847o8u/9b/9ijgMxHZgQY//uScW+c/71upY37pbhiLpoytRMV5JtDF/7eQv2Pn3BtoAHAGUAS8iQbYQPdC/+L/zibWcPmFQC/U6pyF7jF/VMN5RgiINSE2DMMIHbM0DcMw6kDYRNO/f/K5iHztd3f+5j/+goj8KFp+tty/b2cYhtEiCGf0vAw40Tm3Q0SigI9FJLC/9Gfn3MwwPtswDCMshE00nW6W7vC/jfL/2AaqYRgtmrDuafpra5cDucBcf1QT4F7ROuKHRSQmnGswDMNoSBolei4ibdFUhz+iFS6b0DSMqcBa59zdNVwzHn+tckJCwpEHHnhg2NdpGEbrYP162LYN4MutzrnUulzbaClHInIXUOycezDo2FBgonNut/0IBw0a5JYuXRrmFRqGsa9TUQFjx8Krr8Ldd8P9d8nKnc4dUpd7hDN6nuq3MBGROGA4sEpEuviPCXAWsCJcazAMwwhQXg4XXKCC+c9/wp2jM+mifWvrRDij512Aaf7egR7gdefcOyIyX0RS0WYJy9FGu4ZhGGGjrAzOOw/efhsefhgmTAAmZVAJde5uH87oeSbaJ7D68RNrON0wDCMslJRAejrMmQOTJ8M11/j/kJWFrzmJpmEYRlOzcyeMHg3z58Mzz8C44JY1aWl4oM4jY6yM0jCMfZKiIjjtNFiwAF54oZpgAqSnE1kP0TRL0zCMfY6CAhg5Ej7/HKZP1wDQrxgwgBztAFYnTDQNw2haMjMhIwOysiAtTTcgBwyo9+3y8uDUU+Grr+C11+Ccc2o/txjqPILF3HPDMJqOzEx48EFVuu7d9feDD+rxerB1K5x4Inz9terw7gSzvphoGobRdGRkQEqK/ng8Va8z6t7zOjdXBfO77+Ctt2DUqD1fUx9MNA3DaDqysiA5eddjycl6vA7k5MDQobBmDbz7LowY0XBLrI6JpmEYTUdamkZtgiko0OMhsnEjDBmiOvv++3DSSQ28xmpYIMgwjKYjPV33MEEtzIIC3df8VX5Qzaxfry75tm3w4Ydw7LF7uKBa0Cke4uq6ZLM0DcNoOgYMgIkTdR9z40b9PXFiSNHztWvVwszLg7lzQxTMakGn5lZ7bhiGsWcGDKhzitHq1eqGl5TAvHlwxBEhXBQcdAJISalX7blZmoZhtChWrlQLs7wcFi4MUTChxqBTfWrPTTQNw2gxZGZqlFxEBfPQQ+twcQ1Bp/rUnpt7bhhGi2DZMhg+HOLitAHHAQfs5uSaqoxqCDrVp/bcLE3DMJonmZkwaRJccQWfX/U0Jw2tJDERFi8OQTBrqjKCXwWdrPbcMIx9g4DwpaTwP3csI6f9jtSYbcx/ajs9ex+0+2trCPj8cnzSpF2CTsV/+5vVnhuGsQ/gF76FBYdz6vSxdGmzg0XnPUHPJa/t+doGqjKqDRNNwzCaH1lZfLTtcE6bfjE92+az8NIX6N6d0ISvAaqMdoeJpmEYzY73y07kjFcvpm+77Sy4dBpdknaELnzp6bqPmZcHPl/V6/T0BlmbiaZhGM2K2bPhrDcu4uDkbBaM/g8d44rqJnx7UWUUChYIMgyj2TBzJlx4IRxxhIc5D+0k5aOYqrShceNCF756VBmFiommYRjNghkzYOxY+M1vtFtRmzaHwPGHNPWyfoW554ZhNDnTpsEll8Dxx8MHH0CbNk29otox0TQMo0l55hm4/HJt8fbee5CY2NQr2j0mmoZhNBlPPAFXXaWD0GbPhvj4pl7RnjHRNAyjSXj4YbjuOjjzTHjzTa0pbwmYaBqG0ejcfz/ceKNOi3zjDYiJaeoVhY6JpmEYjcrdd8Ntt2lq0auvQnR0U6+obljKkWEYjYJzcOedcO+9mlr03HMQsbvGbDW1dwtT7mVdMEvTMIyw4xzcfLMK5pVXwvPPhyCYNbV3y8xstDXXRthEU0RiReRzEflaRL4Vkb/5j+8nIp+JyBoReU1EWphxbhhGXXAOJkxQzbv2WnjqKfDsSXmC27t5PFWvMzIaZc27I5yWZhlwonPuMGAgMEJEBgP/BB52zvUF8oDQZnUahtHi8PlUKB99FG64AR5/PATBhLC3d9sbwran6ZxzwA7/2yj/jwNOBC7yH58GTAKmhGsdhmE0DV4vjB+ve5e33AL33aezfWqk+v5ldLR2NQo0EIYGbe+2N4R1T1NEIkRkOZALzAXWAvnOuUr/KRuBbuFcg2EYjU9lJVx2mQrmXXeFIJjV9y+zs3WweZjau+0NYY2eO+e8wEARaQvMAg4M9VoRGQ+MB0hrBv93MYxWT4jR7IoK+N3v4PXX4e9/hzvu2MN9axpP0bu3zuhNSalfl6Mw0igpR865fBFZABwDtBWRSL+12R3IruWaqcBUgEGDBrnGWKdhGNQsjvDLzJ5dotnV+lSWl8MFF8CsWfCvf+mf90hWlt4zmORk7YU5aVKDfayGIpzR81S/hYmIxAHDge+ABcC5/tMuBd4K1xoMw6gjtaX6TJmyx2h2aanq66xZ8MgjIQomhH08RUMTzj3NLsACEckEvgDmOufeAW4BbhSRNUB74NkwrsEwjLpQW6rPkiW7jWaXlMDo0fDuu6qv119fh2eGeTxFQxPO6HkmcHgNx9cBR4fruYZh7AW1ucrO1RrN3rkTRo2ChQvh2Wfhiivq+MzAeIrgLYFmsn9ZE1ZGaRhGFWlpauVVF8fBg/U4qIgWFEBeHkUXXMXpI+F//4MXX9QAUL0I43iKhsZE0zBaI7VFwtPTdQ8TdhHHXzYog64pGHMVI27szxdf6KiK889vuo/TmIjmoDdvBg0a5JYuXdrUyzCMfYNAsCcl5dfCOGBASKlF27dr4+Cvv4bXXoOzz26iz7KXiMiXzrlBdbnGLE3DaG3UlBcZOB5wk3fjKm/dCsOHw8qVeskZZzTCmpsR1uXIMFobe1HXvXkzDB0Kq1bpeIrWJphgomkYrY965kX+/LMK5o8/amrRqaeGb4nNGRNNw2ht1CMv8qefYMgQLdKZM0cnR7ZWLBBkGK2ROnRFX79eRXLbFi9zxjzPMe4THerjnNZNNqOu6nXFAkGG0ZoJRQirnzNhwm7Fbs0aFcyi/Eo++u09HJWYD6VRmskOcMIJtdah76uYe24Y+wKhjIeo4wiJVavUJS8uhgUXPcNR++drpH31amjTRn9Wr25WXdUbAxNNw9gXCGU8RB1GSKxYoUGfyko1KgeWf14VcS8ogNhY/QkElJpJV/XGwETTMPYFQkkjCjHV6OuvYdgw1dVFi6B/f3aNuCcna0uj0tJdhbSZdiVqaEw0DWNfIJQ0ohDO+fJLFczYWBXMAwNtw4Mj7v36QWGh/vTr1+y7EjU0JpqGsS8QShpRbef07w+TJrHkjL9z0nGltIkrZ/Fi2H//oPsHOhGlpGhr9qFDdcOzokKPtZIgEFjKkWHsO9Qnet6/P8yezcclRzJy9tV0iitk/kn/IO2vl7cKEbSUI8NozYTSXq36Oddcw8IvEjj9+/H0iNnMvFGT6dbOV1WHbvwKc88No7WSmcnct4o57fuH6RW7iYX9fk+3zPc1wNNKIuH1wSxNw9gXqEOFT4D3/vkN6Zum0i82i48Om0hqdAmUxMLy5TByZCMtvOVhomkYLZ3g/piBpPXbb4cePaCsrEYRffNNGPPq+RzaPpsPO46jvbcMXKyWRm7f3moi4fXB3HPDaOlUT1ovK4O1a2HZshorf954A847D47osol550yh/cGdtOfbihWQnQ2HH277mbvBRNMwWjrVk9ZXrYKkJG2mUa3yZ/p0nUs+eDB8+Ho+bTeuULHs2BH69NHSyNLSWksrDXPPDaPpqcd+5C6kpcH332vDy4ICyMmBDh30B9SKXLmSF1YN5opcx5BBO3n7/UQSE/urC79liwpscjIceSRER1v0fDeYpWkYTUkdm2jUSP/+Opc8P18tTFAB7thRZ1K8/jpTP+nP5bn/5OSET3m3w6UkrvPfv6xMuwmPHq0J6506tao68vpglqZhNCV7mtcTCitWwEEHwbffqtjFxEBcnHbayM/n8crf80f3GKdFzeW/MZcR+7XT4eQDBsC6dSqcweU/raiOvD6YpWkYTclezOv5heXLtbV6p05w6KHQvr32cyss5N+V1/NH9xijPW+T0e5KYj3lapFu2KCWbdeu8Omn8MMPIXdxb+2YpWkYTUlamopUwMKEult6+fka8PF6VQy3bgWvl/u8N3O7u5vzIjOYHnslUaU+TSny+SAiQq854AC9R3a2WqhpaTBunO1n7gYTTcNoStLTdQ8Tdp1BPm7c7q8LBI+WL4dvvtGIt88HiYk4r4+7K29nUuWdXBT9BtM8VxDpHFSiwiqiVmaAvn21rdFzz4XtY+5LmGgaRlMS6B4UHD2vydILjrDHxKg77vNpL7fiYo1+R0TginZwh+8e7qucyGURL/GMG09EQDA9HhXMlBQ46qiqe9seZp0w0TSMpmZ3jTYyM2HyZPjoI92rHDhQk9Zzc7WfZWQktG0LmzfjvD4mRvyHf1dcy3iZypTICXicD8SjLdg7d1axLC3VtCKfL3TL1viFsAWCRKSHiCwQkZUi8q2I/Ml/fJKIZIvIcv/PaeFag2G0aALpSF99Be3aqUX5xhvqkm/cWLWXGRODi4vnevcf/l16LddFTOHJ+JvwRIiKart26o6PGKHX33uvWpsbN7a6XpgNQTgtzUrgJufcMhFJAr4Ukbn+vz3snHswjM82jJZPIB0pUNmTna3C6fPp3qRzsGULvrbtuMY3maluLDfGPMGDEbcgCQl6jx49IDFRr1uyRI+F0kLOqJWwWZrOuRzn3DL/6yLgO6BbuJ5nGPscgXSk5GTYtEm7pHs8Kpp+vF7HuO0PMLV0LLe1eYIH29yNREZodDwgmAFEmuBD7Hs0yp6miPQCDgc+A44DrhORscBS1BrNa4x1GEaLIpCOdOCB6qp7vbof6Z+2UEkElzKNGe5iJkXczV2e/yBDTlJx/OYbFU7n9JqiIq34qc7elnC2QsKe3C4iicB/gQnOuUJgCtAHGAjkAA/Vct14EVkqIku3bNkS7mUaRvMjMNMnJgb226/KJQcqImK5iBnM4GL+4bmDv0bfj6R2UPHLydESyuJibd6xbp1ee9JJu96/IUo4WyFhFU0RiUIFc7pzLgPAObfZOed1zvmAp4Gja7rWOTfVOTfIOTcoNTU1nMs0jOZJ8DCzHj1+SRkqi4jnPN+rvMEYHuImbnP36Tlduuh1W7dqdNzj0Yj7wQfDoEEwe/augliHOehGFWFzz0VEgGeB75xz/w463sU5l+N/ezawIlxrMIwWTyBok5kJCxZQ6o3inIpXeI/TeSz6Jq6reFjP69JF3fK4ONi5U0sj09LU4jzoIC2xzMvbtaY9K2vXJHewZh0hEM49zeOAS4BvRGS5/9jtwIUiMhBwwHrg92Fcg2G0fPxudHF8B84qeZa5DOWp6D8yPnEGFHjUqkxMhB07tD3c1q0aLIqNhZIS+OQTOPZYSE3dVRAbooSzFRI20XTOfQzUFK57L1zPNIx9jsxMuP56dmzawajCGSyqOIrn2kzg8sQM8EZBfLy2g9u2TRPed+xQVzsyUgWyVy8Vz+++U3ENFsT6lnC2cqzLkWE0V/wWZmHOTkbkPMfi4kG81G4Clye8rkEegL/+FY44Quf6BAJFCQkqlqBBoZgYFdTq3YuC90wt0T1krIzSMJorGRnkx3dlRM6tLC06gFcPvpvzEj+DuAPgsMNU5G66CYYPh7Fj9Zr8fHXVO3TQ/c3sbHXXO3asWRAt0b3OmGgaRlMQnB8ZHa1BnLIytQqdg/Jytn++hlM2v0Tmzm7M7DmRsxI/rbIa162Dbt20mXBamg79iYnRe3z6qe5lRkRAz57Qr59ZkA2IiaZhNDbBI3ejomDRIj3ev78KHrDlqNM4ed2zrC7tzKxTn+T0nqWw1Kt7k85pA46kJG3rlpenFqVzOhxt8GCtT9++Xa3Qa64xwWxATDQNoyEJpcImOD9y4UKdAAnwxReQmMimrZGcNGM863w9mN3xKk7ZvhrSBmoqUWKiVvhs3Qrz52tp5SGHQO/eWqOekqLPHjnSqnvChImmYTQUwRZkcIVNddc4OD+yoKBKNLdvJ3tHMifm/5eN3i68d+CNDPMugXyBzz7TiHhUlEbIk5JUPBcv1v3L1FQN5kya1Ogfu7VhomkYDUVGhkawv/5axTA5Wfcdqw9Ji4mBDz5QyzAvT13t+HiySOPE/DfZ7Evlg05jOb7TVsiP136ZvXvD2rUqlOXlamHGxmoUPZBOFB2toml15GHFUo4Mo6EIjJ4oKdFcyR9+gPfeg1deqSpfzMzUruuFhWo1JiRAVhY/ZkczpPh9tvraMTf+LI6X/+l0yZ9/1iBRWprOJ4+NVcuyslIFND5eA0Nr1+q+ptWRhx0TTcNoKIIHnG3cqMeio9WdDghYRoYGa4YMUcGLjuaHjsdxQu4bFNCGed0uZXDMV79Yn6SkaECnf38V2YICFdqOHfWcggL9+6ZN6uZbHXnYMdE0jIaibVstX8zK0lZsublaqbNjhwpcIECUnKyjJ4YOZdVx4xiy6VVKPQkseGUzR7b7US3J+Hh1vQsKdMzuihVw552/NB4mOlpFsksXGD1az/3mG9i8uWo9VkceFkw0DaOhGDhQXeOCAt13FP+4iYoKWLpUU4vWrYOZM2HhQlas9DDkhcvw+WDhFS9y2HkH6GiK/HwVwYQEFdDsbHX9zz0XXnoJxoxRce7YUS3WLl30tcej+5sBrI48LFggyDAaivR0mDVLhTIiokowAdav1/3JIUMgMpLlOZ04+eM/Eh1dwfyjb+NAJ5qo/uOPaiF27Vp13/x8/YGqCp5ABN7jt3sOPFAbc+Tm2sC0MGOWpmE0FAMGaLPg2FgVrspKdadFdJ/T44GNG1na8TROXPc08VLK4t6Xc2BStrrbARHcskXzMJ3ToJLPp65/MGlpKowBOnfWfc+OHa2OPMyYpWkYDcnAgdoko7hYo+SBoWhRUZCSwqc7DmXEN7fTLqqIBQNvpFfBKuh9alV7trS0qlzMqCi1Ovv2hf333/U5NXUoioyERx81oQwzZmkaRkOSnq6J5pWVKnpt22peZkIC/1d5DKf8+CQdZSuLB/6JXmWrNXBTWlp1/UEH6fnx8TBqlDbmiIjYtTsRWIeiJkScf+ZIc2bQoEFu6dKlTb0MwwiNzEyYPBnefFPd8p49mZ8wilGL/0yPiGzmd7qIrt09KpZerwZ8Ro6suv6HHzT407u3JamHGRH50jk3qC7XhOSei0hPYH/n3EciEgdE+sfyGoYRTCAXs7wczj4bfvqJDyJP56x3r6RP1DrmJZ9Dp6RKiOsMhx+u+5bz5mnQJuBmR0SYm92M2aNoishVwHigHTpFsjvwJHDS7q4zjFZH9drzggLeyT2ac768koPa5jC3382k7nfYrvuTeXlw8slVjTbS0jTibYLZbAnF0vwDOjHyMwDn3A8i0jGsqzKMlsbMmbqnGJi5c/TRzCo4kfOXjuOw2O/54Mp3aHfUJToRMtiqzMuzvcgWRiiiWeacK9fhkiAikehQNMNofdTU+u3772HCBK3+EYEtW3jtzRgurriSo+JWMCfhXJLfjoY5UdphvajIrMoWTCiiuUhEbgfiRGQ4cC3wdniXZRjNkID7XVmpgZolSzSZvahIe11G6n9OL1ecz6UVT3GsLOG9uItJiq6A1G5qWb74olb1mFC2WEJJOboV2AJ8g47bfQ/4SzgXZRjNkowMFcxvv9XId2qqWpYbNmjwJjGR58p/x9iyqQyRxcxxp5AkO6pmkicna4WQNdFo0ezR0nTO+YCn/T+Gse8QSpf1YLKy1MKMjVXrcvVqncnj80FREU8mTuSa8ns5JXIes3xnEe+KoeeB2m0dqoTWmmi0aPZoaYrIjyKyrvpPYyzOMMJGwNWuS//JQE/LwkJtvFFR8YtL/mj577lm+72cHjuPt1IuJz7Wp12IAmN1S0pUNLt1syYaLZxQ3PNBwFH+n98CjwIvh3NRhhF2guf0hNp/Mj1dq3yystQddw5KS3kw4hb+xKOczSwyYi8iNsoLRx4Jd91V1cotNlZn+URG/rq6x2hRhOKeb6t26D8i8iVwV3iWZBiNQPCcngCh9J/s1097WzoHHg/3yh38xfs3xkRl8DKXEDVkuNafB1z94cPrtgVgNHtCSW4/IuitB7U8rdGH0bJJS6vKqQywu/6TAXe+Sxdo2xa3PY9J3ju5m7/yu5jXeT75BiIj26pgBg83C7RyM/YZQhG/h4JeVwLrgTFhWY1hNBbVuwStXasW5H77qehVtwgnT9bAT34+rtLLbdzHP7mFy+UFnnbXElEcASNGWJCnFRCKez6sMRZiGI1KoEtQRoZ2Rf/xR91z7Nv316N3MzPh3XehtBSXX8BN3gd4mAlczZM8wXV4ouPUQu3adVfL1dgnqVU0ReTG3V3onPt3wy/HMBqRgOs8aRL07FkleIHfgdG7kydDURG+kjKu9z7ME+4PXM+j/Ic/IR1StWv61q3WKb2VsDtLM2lvbiwiPYAXgU5o2eVU59wjItIOeA3ohd/Vd87l7c2zDGOvCASFNm2CVat0bzMw2TEzE2bOxFe4g9+7KTzDVUyUh3jAcyvi888A2rpVO6ZbDXmroFbRdM79bS/vXQnc5JxbJiJJwJciMhe4DJjnnLtfRG5FK45u2ctnGUb9SUvT+vFvv9XUoDZtVDhzcuCPf8RbVMw49yzTuJQ7+Dv3uDsR59FmwW3bwqBBKpigVmtWlo6vENHkd4ua71OEEj2PBcYBhwCxgePOuSt2d51zLgfI8b8uEpHvgG7AaGCo/7RpwEJMNI2mJD0dLrlERRyczc4AACAASURBVC42VueI5+ZCeTmVOVsY65vGK5zH3dzFnZ57AY/mdsbGVlmYUNUWLipKJ08CnHDCr/dIjRZNKMntLwGdgVOBRWg/zTo1IBaRXsDhaHu5Tn5BBdiEuu+G0XQEBqIlJ+v4idxc6NiRiugELtzxNK9Unsd93Mad3KMlkz6fCuNxx1U1Cw5Oll+9Wq3VNm30dSiJ80aLIZSUo77OufNEZLRzbpqIzAD+L9QHiEgi8F9ggnOuMNBiDsA550SkxjZzIjIebX5MmpWdGeFm4EC1CL/+GpKTKYtpw5jvH2A2I/k3N3AD/1FLNDAexuPR/cyAEAYnywf2RAOvIbTEeaNFEIql6R/cTL6I9AeSgZCaEItIFCqY051zgf/NbhaRLv6/dwFya7rWOTfVOTfIOTcoNTU1lMcZRv1JT1fRzM2lJDKJszP/xuzykTzOH6oEMyKi6nfHjmphfv+9uvaffAIffKCWanKy1pmXlupr2H3ivNGiCEU0p4pICnAnMBtYCfxzTxeJmpTPAt9VS0+aDVzqf30p8FadVmwY4cCft1ncvgdnfn03cwoGMzXhBv4Q93yVWIqohRkRoQKYm6vBIxEdjlZYCAsXQocO+rqwUMsu8/L0x2rO9wlCcc+fd8550f3M3nW493HAJcA3IrLcf+x24H7gdREZB2zAqouMZsKO3gM4o/S//F9xPM8Pn8Glma9ARZzuYbZpA3FxakmKaG7mqlUaDIqNVYEcMgS++kqri4YMqYqed+liHdr3IUIRzR9FZA6aWznfhTjz1zn3MSC1/NmGshnNisJCOO00WPJVIi/9I4uLStfA+jYaSe/RQ6dLFherpdm1K3TuDJ99pmIacMM7d4ZTT9U55E8+2dQfyQgToYjmgcAZ6IC150TkbeBVvygaRosnL0/Lxpctg1dfhXPPTQMmqTt9++3a2q2sTEUxJkaDQe+/r3mcOTkqnEf4+9rY3uU+Tyi158XA66hLnQI8grrqEWFem2GEnW3btHvbihU6UHL0fpkwKaiV22WX6R8D7/v3h2nTdD8zOVlvUFGhLnxg79JKKfdpQmrxJiJDgPOBEcBSbB/SaI7UcXxFbq6OHP/+e3jrLRjZrdrc8rw8HbkbnJQ+aRL07q1NhkH3OJctU1f9rLNs77IVEEpF0HrgK9Ta/LNzbme4F2UYdSazBsGrrQonM5OcaR9y0rMXsX5nKu888RMnj+ytFmZKirri772ne5NeL6xcCc89p/ep3ry4U6eqfczgPprGPksoKUcDnHNnO+deMcE0mi2hjq/IzCT77mcZ+twlZBW35/0zp3Dyx5NUdLOyNKgzfz6sX6/3iY6GH37Qvc3MTLVgAwnrAWwfs1URyp5mYWMsxDD2ihDHV2x44DVOfOfPbKlI4IMBN3FcRy8UeuH66yE/X4M+JSW6T1laqmlDcXF6PCPj182LCwpsH7OVYWMrjH2D4PEVgRZv/hpyMjNhwADWPfE+w165mgKXxNyUMfwmeylML9VyyJQU3eCcMQN27FChjIhQ97yyUgU1K2vX5sWBvVPbx2xVmGgaLZfgwE90tM4kT0rSaHegNjwhAS65hB9iD2XY0gcocbHMT07niJJP9R4ialVGRek1SUlVlmZ0tL73elVIAy64zf1p1VjndqNlUj3wU1Cg+ZPff6+WYceO+rN6Nd+V9ebEb/9NpfOwIPFMBpR+qQIZqNiJiIDUVPjuOxXJ0lLYuRPi4/Ucr1cF1MogDULr3N4PnXk+2/9+FPB5OBdlGHskI0PF8euvVTCTk7VSJzsbzj1XRXHhQr7xHcJJ657A4ytnYacLOMT3g7agiYxUMQQVyvbt9T5duqj4xsZq7iVoUvvxx5t1aQAhdG4XkcXAEc65Iv/7ScC7jbI6w6iN5cth3Trde2zTRl3qFSu03LGgAFJS+CqnM8PXTSaGMuYnnE6/kjXqdjunbrvXqxU+KSlVwtutm7r7Q4fqkLVAoOfaa5v6ExvNhFBSjjoB5UHvy7HGwUZjkZkJY8aotdemjVqCw4apaJaWqmgGItweDyQmQl4en3+XxInrniaBnSyOG0G/TvkqkFFRen5RkYrmsGE6rsI5Fc/994cHHoADDtDcy5QU67hu7EIogaAXgc9FZJb//VnomArDCC+ZmfDHP2rnoMhItRLz8uCLL3S/saBA9xrbt1cB9fmge3c+OfkuRlzVgw7ReSyIO5Oe7XZCl27afGPLFhXC9u1VLMvLNcBz6627CuO55zbd5zaaNaHkad4rIu8Dv/Ufutw591V4l2UY6L7lmjUqkKX+1CARFcfycq3G2bFDrcfkZOjbl8XRJ3PatX3p2hPmz+9M9+t6qNAWFqql2b277oV6vepymwVp1JFQ3HOAeKDQOfcIsFFE9gvjmgxDycrSdmwxMSp0gYg3qDsdE6OCOmoUHHYY83IPZcSrl9Kjh841694dHWMxcCAMHqyWakSEWqfR0Rp9z8xs0o9otDz2KJoi8ld0WuRt/kNRwMvhXJRhAOo2x8drWlBkpFqYFRVqZXq9Kpzx8bBxI3O2HcUZ/3czffp6WLhQtz6BqjEWy5apyILe7/DDbdiZUS9CsTTPBs4EdgI4536mKh3JMMJHerpGsIuLVTB37qxKPE9JUeuzSxfeHngno9/4HQce5GHBAvXafyFQwVNWpmIbFwfHHKOBJRt2ZtSDUAJB5cFTI0UkIcxrMgxlwAB47DEVvf/7v6qhZnFxKoCHHkrGzlM5/4Y0Dj9S55qlpFBzi7izzqoqswxgjTaMehCKaL4uIk8BbUXkKuAK4JnwLssw/AwYAMceq8PK5s9XlzwuDpKSeHVFf3636jKO7vAj78/tq4MfA5VCXq+mDC1ZArNmwdixmtcJ1mjD2CtCiZ4/KCLDgUK0Ougu59zcsK/MMAIW47PPqmXpnEbKKyp4ccMQLi+6g+O7/sg7Y18nKfl2vWbyZE1Rys5Wce3cWQX0xRfhzjt37cJujTaMehBKE+J/OuduAebWcMwwGpaAUC5fDj/+qOMlIiJ0T9PrBa+XZ72XcVXRvxgW/QmzT3iBhAuvr7r2o4903zM2Vo9lZ2uVT6BiyBoFG3tJKIGg4TUcG9nQCzGMX1zrwKwdERW6wMzx6GimVFzJlQUPcWrkfN7p+QcSbru+ylrMyNCk9fJytUijolRwN23ShhwW9DEagN11OboGuBboIyLByWxJwCfhXpjRCgl0Xy8rg9Wr1R2PjNTfnTrxyMZzmFA0iVHt/8cbJ79EzIFn7+peB9zub7/VpPdAPqbXq9amBX2MBmB37vkM4H3gPuDWoONFzrntYV2VsW8R6sCzrCy1DpcsUQsxwI4dPFA+gVvybya9wyJeOf8togvdr1u1xcTAggUqlIWFVd3Xe/RQ8bXWbkYDUKt77pwrcM6tR0f2bnfObXDObQAqReQ3jbVAo4UT7HIHDzyrqRInLU33MmNjNTu9tBTy87mnZCK3bLuZCxLf4dVjHiW6Q5uqJhqZmXD11Vr188oruocJGm0PzCiPjramG0aDEUrK0RTgiKD3O2o4Zhg1EzzwDKp+Z2T8WsTS0+Hll6FdO3AOV+nlrrK/8HfvbVwS8xrPD5lBxN/vqbouM1MHnq1dqz0xy8r0eGC2T2qqtoCLjjbBNBqMUAJB4pxzgTfOOR82JsMIlawszYsMprZKnAEDtLwxOxv3zQpu3aGCOS5+Bs8f8hARfXrtWvaYkaFdi9q00XLKiAh17yMjVUR79VLRrPrX1zD2mlDEb52IXI9al6DBoXXhW5KxTxE88CxAoBKn+l5n//5QWoqLiuYG7794xHc910RM5fGIm/EUdVQLMlhss7LUugyIcps2+iyfT88tKdG+mUOGNO5nNvZpQhHNq4FHgb8ADpgHjA/noowWTrAY7tih+5SRkeoud+umr3/726oZP1FR8P778Mgj+Lr14LrC+5jiu4A/RTzOw3G3I4mJet7y5TAyKNstJkYtzexstShTUrROvaKi6pw+fazrutGghFIRlAtc0AhrMfYFZs6Ee+5R4YqL08a/oO7yli06CjdQmRNIL1qyBGJj8UZE8/t1t/JsyQXc3P4Z7vfcg0QmquXonN4rEAHPzISfftL9yrKyKqsyIUF/Dj5Yg0O1ReoNo57sLk/zZufcAyLyGGph7oJz7vrd3VhEngPOAHKdc/39xyYBVwFb/Kfd7px7r55rN5obgcBMQYEGYnbu1H3GHj30fceOOov88cehbVsVs8WLVTBjE7i8dDIvlZzDnYkP87foB5GuPSAnR+8tAsOH75rI3qePRuSXLtU6c49Ho+7PPWdCaYSN3Vma3/l/L63nvV8AHkfHZQTzsHPuwXre02jOTJ6sohgfry53YaFagWvXaoJ5hw7aty03F37+WY///DMVCW0Zu+NuXt15Kncn/JM7e74E2cXa+q19exXG7GwV0EmT1HrMytLjHg+ccYY+3+dT8TTBNMLI7qZRvu3/Xa95QM65xSLSq37LMlokS5aoa1xeXrW36PXq7wR/R8GsLLU4Kypg+3bKY5K4MPcRMspO5Z/tH+DmYUshK0HPadtWLczsbDjkEO2tGcjzTEj4ZerkL1irN6MR2J17/jY1uOUBnHNn1vOZ14nIWNSCvck5l1fL88fjDzil2X8ITUuoFT0iGpzZtq2qoieQ7hPomg66r9m9O2WRCZy3ZTJvl53Aw4l3MqHne9D1eN0LDSSjT5q0a/Q98LusTI+DtXozGpXd5Wk+CDwE/AiUAE/7f3YAa+v5vClAH2AgkOO/f40456Y65wY55walpqbW83HGXlOXip7Bg6sGmFVW6g+oC11Soi57z55QVkaJL4bR257j7YITmHz8DCYcsVivDYzMBRXM6dM1ar5pU9VzkpPVmp04Uc+3UbtGI7I793wRgIg85JwbFPSnt0WkXvuczrnNgdci8jTwTn3uYzQidanoueYabfgbCAAFZozHxOjvTp3AOXZGp3Dmdw+woHgQz4x6i3FHfA95A3UG+aRJVUKdkgJdu6oV+emnVWMqAm74gAEmkkajE0pFUIKI9A688U+irNfICxHpEvT2bGBFfe5jNCJ1rejp3l2TzOPiNB0IdP8yKgqAoq1lnFY5m4XFRzNt+HTGDfyyqhVcIJ0oWKgPPlhdfBH47rtfn2sYjUwoye03AAtFZB0gQE/g93u6SEReAYYCHURkI/BXYKiIDET3SteHch+jidldRU9NdO+ubvb27SqeFRVqeRYXU3DIsYxc9nc+z0pg+v0buKBkLWRt/HUX9UBkHNQ6PfZYWLlSI+7DhlnHdaNJCSW5fY6I7A8c6D+0yjlXFsJ1F9Zw+Nk6rs9oatLT1VWG0AIuAwfCmjXqknu9GuXu1Im8mM6c+sFEvspN4LXX4JxzegKTdr02EHBatkx7Yh5xhIpmp05qtQbcd8NoQkIZdxEP3Aj0dM5dJSL7i0g/55ztR7YGAiNwg6Pn1S294Oh6URGsX6+CCVBYyNZtwnDv46ws60jGmzBqVA3PCd7H/M1vNOl94UI44QRtFWeRcaOZEIp7/jzwJXCM/3028AYWxGk97C7gUr1scsMGjZT7fCBCrkvlZM/bfO/2461e1zOi5zVADfeqHnAaMkQHpH3+OYwebS650WwIRTT7OOfOF5ELAZxzxSIiYV6X0RKYORP+8AcVzDZtdM+xqEgj586RI105yX3Iel8v3m17MScdE1Nz1B123ccEjZKfeqqmE5lLbjQjQomel4tIHP5EdxHpA+xxT9PYx8nMrLIwExP1d1GRWpiVlWx03Rjim08WabwfMYqTOq3QWvHahpulpel+aTBW4WM0Q0IRzb8Cc4AeIjIdbQ13c1hXZTR/MjKqRHLrVo2Y+3wArHc9OYFFbKYTH0aPYkj0p7vmV9ZEenpVOpHPZ6lFRrNlt+65iHiAFCAdGIymHP3JObe1EdZmNDahlksCLFoEmzdr1Y/PpxU6wFp6cyLzKaQNH3lO5aiKz0Gi9Z55edoWriZCCTgZRjNA3B5GAYjI0moVQY3OoEGD3NKl9W22ZIREcPQ6OLWoptLEzExt01ZWpm65zwdlZax2+3MS8yghno9iz+Dwyi80it61q7rmgQbEVu5oNBNE5Mu66lsogaCPRGQi8BqwM3DQxvjuY9SlXDIjQ9u/wS9NgFeW9+VENxefRLCw84UcWrYaymO1W9Ell1Rdm5dXezDIMFoAoYjm+f7ffwg65oDeNZxrtFSqR6+h5nLJmTPhySerGg0nJJAZeQQn8wYRVLCwzWgO7l4J9IHVqzWSvmmT7mnWdk/DaEGEUhG0X2MsxGhiQimXnDkTbr5Za8ETE6GoiGX5vRnum0Wcp5T5kadwgGyE1b6qPprZ2ZqoPmZMzfc0jBbGHqPnIhIrIjeKSIaI/FdEJohIbGMszmhEQoleP/645mP26AEeD59HH89JlXNI9BWw2DOMA5I3a8ljaakGiBIT9feaNToTyCLixj5AKO75i0AR8Jj//UXAS8B54VqUESZ2Fx0Pjl4vX66Ngtu23XXO+DffaCAnLo7/RQ5hZNEzpMpW5nuG07NXJOSL3jsyUntoer3qju/cCR9/DFdfbRFxo8UTimj2d84dHPR+gYisDNeCjDARHB0PbiYcHMkO/F63TpsFJyfreXfcoS55bCxUVrJwxyDO2PYc3SI2Ma/tuXQv3Qydj9Sg0NatWk7p8WgaUkwM9OunOZ1W2WPsA4QimstEZLBzbgmAiPyG+g9bM5qK3UXHA7+zslQwu3XTdKLFi3UPMjtbRTM+no+2HMaZFS+yn6zno+hRdPFuV6u1tBTatVPhdE73M6Oj1ZWvrNR7GsY+QCiieSTwiYgEQp5pwGoR+QZwzjnztVoCtUXHly9XoaysVHH8+mu1SpOSNOLt8fwSKX8/+QLOrvgXB/ADHzGcjhXb4LihmoP56acqtL16aQ26xwP77af3LSyEv/ylKT61YTQ4oYjmiLCvwgg/tUXH8/M15/Lbb9X9Tk6GLVvUckxIUAH0epntRnHe2gc5JHI1c9uOob2rBJK1oUbv3jot8ttv1RXv3x9+/FHv362bCua55zbZRzeMhiSUlKMNjbEQI8zU1ky4bVu1MGNjdS8yIUHf+3zwww8gwkzPGC6smMYRLGNO5Nmk7MxXF/z449WKDLRvu/VWC/IY+zyhNOww9gUC0fHq0xsHDlTLMjYWduzQMRX+eT74fMyoHMMFFS9ytHzBXIaTUr5ZXe+kJMjN1esPP1yDPCaYRivARLO1k56uIllQoJFv5zSAI8I0N5bfuRc5no/5QEbSxuOvoo2JUXc/Nlb3RC1Z3WhFmGi2FmqbXw7aecg53d8UARGelvFc7p7lJObznmcUiZGlVfmXERHqxjsH27ZZsrrRqjDRbC0Epxx5PFWvMzLggAN0pnhsLBQW8oT3asZ7pzDC8yFvx40hPsZbdZ/kZN3vLCxUgR0+3Nxyo1URSvTcaMkEqoCmT9cWbQcfrKWOsGvKUUoKjB7Nw9PacWPJfZyZvJDXo64kpqhYK3s8Hi2L9FuiHHywWpzXXNO0n88wGhkTzX2Z4Cqgrl113/KTT3SOeKdO+n7jRhW/8nLuL7iG28rGc07MO8yIuYboHp1hXXGVO56YqEGjxESNsD/6qFmZRqvD3PN9mWCX/KCDdA9SBFau1D3Ndeu0+7pz3J33R277cTwXJr7Nq90nEt0+CU47TVOSPB4N/nTooPuXF1+suZkmmEYrxCzNfZGaXPLOnXXf8rvvNPFcBH7+Gef1cef6K7l3yxWM7TSH59L+QURJFLRvr1ZoWpre44ADqu6fl2cRc6PVYqLZ0qneuah/f5g2TXMoi4pg1SrIyYGTTlLhLCrSKPlhh+Hy8rl5+608uO0KrkyZyVMHPI6n1KtzywOud3DUPTgpfty4pv7khtEkmHvekqkpjej227V3JWgJo9er+5Cff65/X7ECDjkE1zaFCdv+woPbruDalFd4KunPeIr83dhPPvnXLeOqJ8Wba260UszSbMnU1LkoL0+T0+Pi9FivXmpprl8Po0bBfvvh67M/1757Bk9lD+KGDi/xUK/HkIpkOOEEvf7aa3d9zoABJpKG4ccszZZMVpa6zMEE+liClkVu3arvo6Kgf3+8eYVc+dhhPPXlIG49fA4Ppf8P8fjTiMyKNIw9EjZLU0SeA84Acp1z/f3H2qFTLXsB64Exzrm8cK1hn6emzkUdO6qYrlypCegREVrJk5pK5Z9u4vLCR3h5x1ncFfkPJn1/H1LRS63Re+81sTSMEAinpfkCv24rdyswzzm3PzDP/96oLzXN9UlJ0cqenTs1xai8HHbupCIrh4tz/sXLO87i7zH38Lfoe5GK8l9SjgzDCI2wiaZzbjFQfTb6aGCa//U04KxwPb9VUD1IU1YGxcWafO7z6U90NOWeWM6veInX3Rj+JX/mDs996ta3aaPnr1sH11+vgSXDMHZLY+9pdnLO5fhfbwI6NfLz9x0yM7Ud23/+o+9PO00FcPt2HTVRXg6VlZQWe0mveJVZpPMI1zPRPaiWZWGhWqM+nyat5+ZqJN6E0zB2S5NFz51zTkRq9QtFZDwwHiDNEql3zceMiYGfftIxE4FUo3vu0dc7d+qICecoIZazeJMPOZUpXM3VPKX3KitT4YyIUCu1rEz3QgMNPGxv0zBqpbEtzc0i0gXA/zu3thOdc1Odc4Occ4NSU1MbbYHNkur5mMuWwdq1KnaBjkUVFZqDGRMDeXnsdHGczrvMZTjPckWVYELVHqbPp2Mt8vLgwAPVZc/KqnkNhmEAjS+as4FL/a8vBd5q5Oe3TKq3dSsv187pq1ZVnZOaqknsmzZR5BIYyfssYggvMpYreP7X9xTRNCSvV6PrnTtrtY9Z9YaxW8ImmiLyCvAp0E9ENorIOOB+YLiI/ACc7H9v7Inq+ZiB1wUFVce6dYPKSvK9SZzCh3zCsczgIn7H9F3vJaKJ7507qwgnJWkj4UAU3hoKG8ZuCduepnPuwlr+dFK4nrnPUj0f86CDYOFCjX77fCqekZFsj+/OqQWv8TWH8QbncTZv/vpe0dF6Tdeu+j4nR2vNU1K0ntz2Mw1jt1hFUEugej5mdDT07asDzfz14FvH3cJJ5e+TyQAySP+1YIrodaDWZUKCBoLatIExY2wwmmGEiNWeN2eCI+bx8bqXGWjXFlTBs3mzNjFaW1bObBnNqfIh+ILuI6IiecIJ8PXX2iOzoECDRn36/LrW3DCMWjHRbK4EIuZerwrlli0auLnzTjj33F9O+/lnFcysLHh3+H84ccN62BCjkXXQwJHHo+74UUfB5ZdrlD3QSi493SxMw6gDJprNlYwMFcwVK7QsMjVVrcN77vmlIfBPz3/Eic9ezKaytsx56id+u74Y2h+hbnhsrP4UFGiK0UsvVYljkOgahlE3TDSbK1lZamHGxla1eUtOVotzyhTWb45j2Lw72F6ewIejJ3PM/K/gzDO1JLJ//19bp2ZNGkaDYKLZXElLgyVL1MIEbfOWkwNeL2ve+IoTS96hyBfPvLEvMqhrAeSlqFU6caJaqTExMGyYud+G0cCYaDZX0tNh1ix1ryMiYMMGAFalHMNJa5+izMGC+JMZ+GE2HH20VvRkZVnDYMMIM5Zy1FwZMEDdaufU5S4vZ0XZ/gxd+wyVLoKFUacwMHKFlkHOnQtffmnVPIbRCJhoNmfOPVeFU4SvPYczrPgdPM7HosiT6e9ZqfXmMTFaBvnll1bNYxiNgLnnzY1Abuby5To18uef+TLiaIYXvkZCVBnz3XD2j94AXo9GycvKNIfT5zO33DAaARPN5kRwbua6deDxsCSnJyN2vkZbl8+C+DPZr+wHKEcT1vv21YbD+fmasG4YRtgx0WxOTJkCq1dr0Ccigo+TRjBy5yN0YjPzk88mzW1QV7y4WH9v2aLNhCsr4S9/aerVG0arwESzuZCZqQGddu0AWFB2LGf8/Bg9PNnMizyVbgllUBKheZciGiDKz9cczj//2RLWDaORMNFsagJ7mG+9pd2Giov50A1n9Lap9I7MYl7kCDq3LQX8fTSTk6FnT93DHD1am3gUFTX1pzCMVoOJZrgIbrZRW433zJlaFllRoT0to6N5b3Uf0iufpp98z0eJ55JakgNJaWpRJiVBp06aZhRcJWTd1g2j0TDRDAeZmXDHHTqsrKwMvv1WU4KCZ4tnZqpgimjVz9atvLlpMGMqp3OoZyUfxoyifUG2uuE//6xt4DyeqlryI47Q+1i3dcNoVCxPMxxMmQJr1ujrQJf1NWv0eICMDLUwk5NBhDcqzuK8yhkc4VnOvMTRtPfmakpRp07aoWjFiqpZ5v37q9Bat3XDaHRMNMPBkiXqSsfFVY2XSErS4wGyslT4tm9n+orDuCB/CoM9X/BhxGm0LcnR6HggjahrV93HTEzUbkX77/9L82EmTrT8TMNoRMw9DweulsnEwcfT0iAnhxe+OIQrSh5miCzmbd8oEqVEBbN9e83XjIrS89u0gexsqy03jCbGLM1wMHiwRrRLSlQoS0r0/eDBVeekpzN12SAuL5nMyczjXRlFoqdYW8H5fFX5lx066PmFhTo8zTCMJsVEMxycfHJVVc+KFfDjj9raTUQDQMDjr7Tn97n3cFrEHGbHjiE+HnW/k5LU0iwtVfc8Pl7zMQsL4brrmvZzGYZh7nmDk5kJs2fraIlVqzQAJAK/+Y0K3yWX8JB3AhO/vZzRSfN5LXocMQlt9JyKCr3HUUdppNw5dcm7ddOKH0tgN4wmx0SzIcnMhOuv11Sjjh1VCA88UP+2fj1UVHBf7jhu33Q55yXNYXrHG4jKK4XSSHXLQV35bt10pMWkSU31SQzDqAVzzxuKQLON3FzdhywpUfe8shJiY3E/beRv2/7A7Zuu56KU95hx+L+ISozRlCLQenLQSHlkpKURGUYzxSzNhiIjQ1OAOnZUwYyLU/H7/ntcTCx3FN7MfRXjAKWorwAAEDBJREFUuSzyZZ4p/j0RPyTr3xMT4ayztBXctm3w29/CNddYhNwwmikmmntDcKnksmW6b3nQQfDJJ2o5VlTgysqZWPlP/l1xLeOZyhS5Hk/bthooKixU0ayogJEjbZ6PYbQATDTrS8AdT0mB7t21VHLxYhgyBPr1gw8/xLejmD+5h3m84lqukyd41F2HRMZpt/XKSs3BjI+H555r6k9jGEaI2J5mfQm44ykpGukO1IL/73+wahU+r+MaN5nHfX/gprjJPBpxAxIToxHxsjIVzP3209pxwzBaDCaa9SUrq6quHDSgc8IJsGUL3gof48omM9V3Jbe1eYJ/tb0XifB/1TExGlHv1UuT2C1h3TBaFOaeh0r1Vm/R0WolpqRUnRMbS2Wnblzqe44ZFYcxKeEB7kp4BIn0Nw8uK1OhDFT8FBZax3XDaGE0iWiKyHqgCPAClc65QU2xjlqpLpD9+2vCemD/Mi9Pk86dgz591OIsKKBizQYu/ul+3sg7jH+0fYDb2k+FyijtTtS+PfToAT/9BDk5lrBuGC2UprQ0hznntjbh82umeoAnL0/7XvbvX2VVpqRA797aST0lBbKyKItM4PwlN/FW3m95KO4v3OiZApvLoUsXbbbRv7+mGD3xhEXIDaMFY+55dYIDPKC/Kyq0Fdv++8PmzfDdd1oPLgLTplF6wADO6fcN720/lMd6/ovr2r8Pm2LVHS8qgoMP1gofSykyjBZPU4mmAz4UEQc85Zyb2iSrqGkkRVaWWpjBpKbq5MfNmzUHMzZW9zRFKL7/Uc768WHmZh3KUz3/wfheHwJJ2nijuFit0TffDO3ZJqiG0ewRV1vvx3A+VKSbcy5bRDoCc4E/OucWVztnPDAeIC0t7cgNGzY07CKC3XD/niR5eZo3WVioIyYKCvRvcXH6PjZW9zFFoLSUHYOGMuqD61iU3Zdne0zi8s7v6/UB/AnufPVVaM+2hsKG0aiIyJd1jak0ScqRcy7b/zsXmAUcXcM5U51zg5xzg1JTUxt+EdXzLAOvt23TDuv5+Wot5ufrLPKxY9XdLi+HuDgKjxzGiLk3sfjnPrx0wtNcfnrunnto7unZGRkN/zkNw2hQGt09F5EEwOOcK/K/PgW4u7HXUaMbnpwMixbBMcdodLygQHtaHnKICuBZZ0FeHvlxXRjx8u/4MqcLr46YxnlH/wzp1+i+Z26uXhcTA337ah15qM+2qZKG0expij3NTsAsEQk8f4Zzbk6jryItTV3i4DzLggJ1vfv00aBPAJ9PBW3CBLbfO4VTFl5E5tbOvDHyec5qtxjS/W71vfeGtk9Z27NtqqRhNHsaXTSdc+uAwxr7ub8iPV33FWHXfcX994dZszTwA2oR9usH++/Pli4DOHn5v1m9LYpZJz7O6YPyqwQTQp/fU9uzx41r+M9pGEaD0nrLKAcM0MBLSkrVZMczz4RNm9RSdE73G9euhUWL2NTtSIYOhe+z4pj9XiSnz52gTYLrE7ip6dkWBDKMFkHrztOsbhlOmqTBnh49dKZPaSnEx5OdcAAn3nYMG0vgvfdg2LAwPNswjBZB6xbN6mRlqWi2b6/pRVu3krWjHSdufpbNJPDBM2s4ftHL8JLlVhpGa6X1uuc1kZamUe/t2+Gnn/ixpDND8t9kq68dc7uM5fj/3qB7j4Hyygcf/GW6pGEYrQMTzWDS03VcRU4OP5T24IQtMynwJjIv6WwGxy7XdCLLrTSMVo2JZjD+tKHvYg5jSOHblLoYFrQ/jyN75GrSe37+rudbbqVhtDpa955mDfXfKzwDOGnLG0ikj4UDJ3JIghfoAFu3anAoGMutNIxWR+sVzUD9t9eraT9LlrB8xkpO3vQSMRGO+WlX0M+zGVysRtGTkvR3Xp7lVhpGK6b1imZGhrrcS5eCz8fSiN9wypapJEZsZ/45k+krifBzYVXTjqOO0r6Y/v6ZpKWpYFr03DBaFa1XNBctgs8+A+f41HMcI0pm0E7yWND5Enq1PQR2RsJhh+1qVV57rYmkYbRyWmcgKDMTVq4Er5fFnqGcUjyLjuSyOOkMenn8uZpWsWMYRg20TkszIwPi45lfOIhRxa+T5tnIvOjT6FqxCaSjut5WsWMYRg20TkszK4sPEs/h9LL/sp9sYGH0qXSN3goREbpvmZ7e1Cs0DKOZ0iotzXdKT+acledxUMxa5vb8PamlQKF/zO6dd5qFaRhGrbQ6S3PWLEh/4wIGpPzE/NP/TWonj46o6NFDJ0XaSF3DMHZDq7I0X3sNLr4YjjrKw5wHi0me2xWyKrVtkTXfMAwjBFqNaL78Mlx6KRx7rLZ3S0rqD8f1b+plGYbRwmgV7vlzz+lctCFDYM4cLe4xDMOoD/u8aD75pBbuDB8O77wDCQlNvSLDMFoy+7RoPvqoDoM8/XR46//bu/sgq+o6juPvjwisiY4RDENlIdSg1BgSOJYPPVANkSKMDBFjk9qEODzFCCMMlugMM0AGOqE0YDxEBITkQGSmEuqYMwoizwqi0hRDkGMSmGyw++2P32/hsNy7u3dX7vnd9fuaubPnnnvOng8/dr97zj33fM+a029J7pxzzdFqi+b998P48TBkSPgse1VV3omcc61Bqyya06fDpEkwbFg4Y96uXd6JnHOtRasqmmZwzz1w991w882wbFn4vLpzzn1QWs1HjsxgyhSYORNuvRUWLAhXRTrn3AepVexpmsGdd4aCOWoUPPKIF0zn3NlR8UWzthbGjoU5c2DcOHj44XDfM+ecOxsq+vC8thZuvz3sWU6cCLNmgZR3Kudca1ax+2Q1NXDbbaFgTp3qBdM5Vx4Vuad54kS4LHL5crjvvtDNzTnnyiGXPU1JAyTtlrRX0uRS1j1+HIYPDwVzxgwvmM658ip70ZTUBngI+DbQC/iepF5NWbe6OrS7XL0aZs+Gu+46m0mdc+5MeexpXgnsNbM3zex/wArgxsZWev/9cEnk2rUwdy5MmHDWczrn3BnyKJqfAP6eef6POK+o2loYNCi0dZs/H0aPPqv5nHOuqGRPBEkaCYwEaN/+co4fh0WLQiNh55zLSx57mvuBizPPPxnnncbM5ptZXzPrW13dlqVLvWA65/InMyvvBqVzgT1Af0Kx3AiMMLOdDazzL+BvQCfg7XLkbIaUs0Ha+VLOBp6vJVLOBtDTzEq6l0PZD8/N7ISkMcCfgTbAwoYKZlynM4CkTWbWtwwxS5ZyNkg7X8rZwPO1RMrZIOQrdZ1c3tM0s8eBx/PYtnPOtUTFXkbpnHN5qLSiOT/vAA1IORuknS/lbOD5WiLlbNCMfGU/EeScc5Ws0vY0nXMuVxVRNFvS4KMcJO2TtF3SluacjTsLeRZKOiRpR2ZeR0lPSXo9fv1oQtmmSdofx2+LpIE5ZbtY0gZJuyTtlDQ+zk9l7IrlS2X8qiS9JGlrzHdvnH+JpBfj7+9KSWW/1WED2RZLeiszdr0b/WZmlvSD8LGkN4DuQDtgK9Ar71z1Mu4DOuWdI5PnOqAPsCMzbxYwOU5PBmYmlG0aMDGBcesK9InTFxA+T9wrobErli+V8RPQIU63BV4ErgJ+BwyP838J3JFQtsXA0FK+VyXsaTarwceHmZk9B7xTb/aNwJI4vQQYXNZQUZFsSTCzA2a2OU4fAV4l9EVIZeyK5UuCBUfj07bxYcDXgUfj/FzGr4FsJauEollyg48cGPCkpJfjNfMp6mJmB+L0P4EueYYpYIykbfHwPZfD3yxJ3YArCHskyY1dvXyQyPhJaiNpC3AIeIpwlPiumZ2Ii+T2+1s/m5nVjd30OHZzJLVv7PtUQtGsBNeYWR9Cj9DRkq7LO1BDLByjpPSxiXlAD6A3cAD4eZ5hJHUAVgM/NrP/ZF9LYewK5Etm/Mysxsx6E3pKXAlcmleW+upnk/R5YAohYz+gI9Bol95KKJpNavCRJzPbH78eAh4j/LCk5qCkrgDx66Gc85xkZgfjD3QtsIAcx09SW0JBWmZmv4+zkxm7QvlSGr86ZvYusAH4EnBR7DkBCfz+ZrINiG95mJlVA4towthVQtHcCHw2noFrBwwH1uac6SRJ50u6oG4a+Bawo+G1crEWqOsT9QNgTY5ZTlNXkKIh5DR+kgT8CnjVzGZnXkpi7IrlS2j8Oku6KE6fB3yT8L7rBmBoXCyX8SuS7bXMH0MR3mttfOzyPNtWwpmvgYQzhW8AU/POUy9bd8IZ/a3AzhTyAcsJh2nHCe8h/RD4GLAeeB14GuiYULalwHZgG6FAdc0p2zWEQ+9twJb4GJjQ2BXLl8r4XQ68EnPsAH4a53cHXgL2AquA9gll+0scux3Ab4hn2Bt6+BVBzjlXgko4PHfOuWR40XTOuRJ40XTOuRJ40XTOuRJ40XTOuRJ40XTJit17JhaYP1hSr2Z8v26SRmSe3yJpbktzFtjOM5KSvS+Oaxkvmq5FMld6lNNgQnefMzSSpxswooHXnWuUF01XlKSfxD6mz0taXrfXF/ekHoi9Q8dL6i/pFYWeogvrmh4o9BntFKf7SnomTk+Lyz0j6U1J4zLbnCppj6TngZ4FMn0ZGAT8LPY/7FEgz2JJQzPr1HW3mQFcG9ebEOd9XNITCr0yZxXY3gBJqzLPvyppXZyeJ2lTtj9jgfWPZqaHSlocpztLWi1pY3xc3fD/hktFLnejdOmT1A+4CfgCoY3WZuDlzCLtzKyvpCrClTL9zWyPpF8DdwAPNLKJS4GvEfpC7pY0j3DVxnBC44lzC2wTM3tB0lpgnZk9GrOezBOfLy6yzcmEvpPXx+Vuidu6AqiOOX5hZtmuWk8D8yWdb2bvAd8ltCeEcPXXO5LaAOslXW5m2xr5d9d5EJhjZs9L+hThltaXNXFdlyPf03TFXA2sMbNjFno3/qHe6yvj157AW2a2Jz5fQmg03Jg/mlm1mb1NaIDRBbgWeMzM/muhe08pPQZWNr5IQevN7LCZHQN2AZ/OvmihpdkTwA3x0P87nLp2epikzYTL8z5HkbcMivgGMDe2KlsLXBi7F7nE+Z6ma673mrDMCU79Ya6q91p1ZrqGlv8sZvOc3K6kcwgd/4tpSo4VwBhC8+RNZnZE0iXARKCfmf077t3W/zfC6W3ksq+fA1wVi7WrIL6n6Yr5K2HvqiruAV1fZLndQDdJn4nPvw88G6f3AV+M0zc1YZvPAYMlnRc7R91QZLkjhMP6YrLbHUR4e6Ep6xXzLOEWHT/i1KH5hYRCfVhSF0Iv1UIOSrosFu8hmflPAmPrnqgp96ZxSfCi6Qoys42Ew8ZtwJ8InWAOF1juGHArsErSdqCWcB8YgHuBB+MJmpombHMz4TB7a9zmxiKLrgAmxZNPPQq8vgD4iqSthH6OdXuh24AahZtrTSiwXrFcNcA6QmFcF+dtJRyWvwb8lvBHppDJcZ0XCN2d6owD+ip0DN8FjGpqHpcv73LkipLUwcyOSvoIYS9wZCxszn1o+XuariHz44fIq4AlXjCd8z1N55wrib+n6ZxzJfCi6ZxzJfCi6ZxzJfCi6ZxzJfCi6ZxzJfCi6ZxzJfg/w42FFOcNc4MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQikz3IPiyPf"
      },
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "4f3fd2b6-eb68-45c4-a9be-fb1a109d4079"
      },
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    print('Saving results to {}'.format(file))\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])\n",
        "\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to pred.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tmCwXgpot3t"
      },
      "source": [
        "# **Reference**\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
      ]
    }
  ]
}